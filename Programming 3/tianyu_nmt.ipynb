{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjPTaRB4mpCd",
        "colab_type": "text"
      },
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignmentby selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9IS9B9-yUU5",
        "colab_type": "text"
      },
      "source": [
        "## Setup PyTorch\n",
        "All files are stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axbuunY8UdTB",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-6MQhMOlHXD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "a9b31a45-6a61-41ec-b981-e2192e40061e"
      },
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install torch torchvision\n",
        "!pip install Pillow==4.0.0\n",
        "%mkdir -p /content/csc421/a3/\n",
        "%cd /content/csc421/a3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Collecting pillow>=4.1.1\n",
            "  Using cached https://files.pythonhosted.org/packages/19/5e/23dcc0ce3cc2abe92efd3cd61d764bee6ccdf1b667a1fb566f45dc249953/Pillow-7.0.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pillow\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-7.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting Pillow==4.0.0\n",
            "  Using cached https://files.pythonhosted.org/packages/37/e8/b3fbf87b0188d22246678f8cd61e23e31caa1769ebc06f1664e2e5fe8a17/Pillow-4.0.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow==4.0.0) (0.46)\n",
            "\u001b[31mERROR: torchvision 0.5.0 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: scikit-image 0.16.2 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "Successfully installed Pillow-4.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content/csc421/a3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DaTdRNuUra7",
        "colab_type": "text"
      },
      "source": [
        "# Helper code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIpGwANoQOg",
        "colab_type": "text"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-UJHBYZkh7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "    \n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "        \n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel('Epochs', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
        "        pkl.dump(idx_dict, f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MglI7o8xs1Pk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4cd4dd5e-b7db-43e6-a31d-21d947e300fa"
      },
      "source": [
        "torch.cuda.get_device_name()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbvpn4MaV0I1",
        "colab_type": "text"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVT4TNTOV3Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\n",
        "    \"\"\"\n",
        "    lines = open(filename).read().strip().lower().split('\\n')\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n",
        "    \"\"\"\n",
        "    return all(c.isalpha() or c == '-' for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n",
        "    \"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n",
        "    \"\"\"\n",
        "\n",
        "    source_lines, target_lines = read_pairs('data/pig_latin_data.txt')\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index['SOS'] = start_token\n",
        "    char_to_index['EOS'] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = { index: char for (char, index) in char_to_index.items() }\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = { 'char_to_index': char_to_index,\n",
        "                 'index_to_char': index_to_char,\n",
        "                 'start_token': start_token,\n",
        "                 'end_token': end_token }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s,t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s,t))\n",
        "\n",
        "    return d\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRWfRdmVVjUl",
        "colab_type": "text"
      },
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa5-onJhoSeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\n",
        "    \"\"\"\n",
        "    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\n",
        "    \"\"\"\n",
        "\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "\n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "      \n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "    \n",
        "    if isinstance(attention_weights, tuple):\n",
        "      ## transformer's attention mweights\n",
        "      attention_weights, self_attention_weights = attention_weights\n",
        "    \n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "    \n",
        "    for i in range(len(all_attention_weights)):\n",
        "      attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "      fig = plt.figure()\n",
        "      ax = fig.add_subplot(111)\n",
        "      cax = ax.matshow(attention_weights_matrix, cmap='bone')\n",
        "      fig.colorbar(cax)\n",
        "\n",
        "      # Set up axes\n",
        "      ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n",
        "      ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n",
        "\n",
        "      # Show label at every tick\n",
        "      ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      # Add title\n",
        "      plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n",
        "      plt.tight_layout()\n",
        "      plt.grid('off')\n",
        "      plt.show()\n",
        "      #plt.savefig(save)\n",
        "\n",
        "      #plt.close(fig)\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
        "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n",
        "            \n",
        "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, encoder_hidden)\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "              # Zero gradients\n",
        "              optimizer.zero_grad()\n",
        "              # Compute gradients\n",
        "              loss.backward()\n",
        "              # Update the parameters of the encoder and decoder\n",
        "              optimizer.step()\n",
        "              \n",
        "    mean_loss = np.mean(losses)\n",
        "    return mean_loss\n",
        "\n",
        "  \n",
        "\n",
        "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
        "        \n",
        "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
        "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, train_loss, val_loss, gen_string))\n",
        "\n",
        "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        save_loss_plot(train_losses, val_losses, opts)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Data Stats'.center(80))\n",
        "    print('-' * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print('Num unique word pairs: {}'.format(len(line_pairs)))\n",
        "    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n",
        "    print('Vocab size: {}'.format(vocab_size))\n",
        "    print('=' * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data()\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    if opts.encoder_type == \"rnn\":\n",
        "      encoder = GRUEncoder(vocab_size=vocab_size, \n",
        "                          hidden_size=opts.hidden_size, \n",
        "                          opts=opts)\n",
        "    elif opts.encoder_type == \"transformer\":\n",
        "      encoder = TransformerEncoder(vocab_size=vocab_size, \n",
        "                                   hidden_size=opts.hidden_size, \n",
        "                                   num_layers=opts.num_transformer_layers,\n",
        "                                   opts=opts)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if opts.decoder_type == 'rnn':\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, \n",
        "                             hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == 'rnn_attention':\n",
        "        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n",
        "                                      hidden_size=opts.hidden_size, \n",
        "                                      attention_type=opts.attention_type)\n",
        "    elif opts.decoder_type == 'transformer':\n",
        "        decoder = TransformerDecoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    #### setup checkpoint path\n",
        "    model_name = 'h{}-bs{}-{}'.format(opts.hidden_size, \n",
        "                                      opts.batch_size, \n",
        "                                      opts.decoder_type)\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n",
        "\n",
        "    try:\n",
        "        training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts)\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return encoder, decoder\n",
        "      \n",
        "    return encoder, decoder\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yh08KhgnA30",
        "colab_type": "text"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aROU2xZanDKq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d2d631ba-888f-4aa4-8360-1768d8066c50"
      },
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(fname='pig_latin_data.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_data.txt', \n",
        "                         untar=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/pig_latin_data.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDYMr7NclZdw",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Gated Recurrent Unit (GRU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCae1mOUlZrC",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: GRU Cell\n",
        "Please implement the Gated Recurent Unit class defined in the next cell. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HMO7FD6l5RU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyGRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyGRUCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        ## Input linear layers\n",
        "        self.Wiz = nn.Linear(input_size, hidden_size, bias=False)\n",
        "        self.Wir = nn.Linear(input_size, hidden_size, bias=False)\n",
        "        self.Win = nn.Linear(input_size, hidden_size, bias=False)\n",
        "\n",
        "        ## Hidden linear layers\n",
        "        self.Whz = nn.Linear(hidden_size, hidden_size, bias=True)\n",
        "        self.Whr = nn.Linear(hidden_size, hidden_size, bias=True)\n",
        "        self.Whn = nn.Linear(hidden_size, hidden_size, bias=True)\n",
        "        \n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"Forward pass of the GRU computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        r = torch.sigmoid(self.Wir(x) + self.Whr(h_prev))\n",
        "        z = torch.sigmoid(self.Wiz(x) + self.Whz(h_prev))\n",
        "        g = torch.tanh(self.Win(x) + r * self.Whn(h_prev))\n",
        "        h_new = (1 - z) * g + z * h_prev\n",
        "        return h_new\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecEq4TP2lZ4Z",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: GRU Encoder\n",
        "Please inspect the following recurrent encoder/decoder implementations. Make sure to run the cells before proceeding. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jDNim2fmVJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GRUEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(GRUEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = MyGRUCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n",
        "            hidden = self.gru(x, hidden)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvwizYM9ma4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None        \n",
        "        \"\"\"        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSDTbsydlaGI",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "Train the following language model comprised of recurrent encoder and decoders. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51HladZJtG7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3YLrAjsmx_W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "927ec8a6-a7f8-473b-f6bf-90b279d57ad3"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_encoder, rnn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 20                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('contrived', 'ontrivedcay')\n",
            "('gilberts', 'ilbertsgay')\n",
            "('agitated', 'agitatedway')\n",
            "('strictest', 'icteststray')\n",
            "('gaining', 'aininggay')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.297 | Val loss: 2.084 | Gen: ontay-ay ansay-ay interay-ay intay-ay intay-ay\n",
            "Epoch:   1 | Train loss: 1.847 | Val loss: 1.898 | Gen: ayday ayday onteredsay insay-ayday onteredsay\n",
            "Epoch:   2 | Train loss: 1.708 | Val loss: 1.829 | Gen: ellay-ayday-ayday-ay ayday-ayday-ayday-ay onteredsay-ayday-ayd insay-ayday-ayday-ay onteredsay-ayday-ayd\n",
            "Epoch:   3 | Train loss: 1.613 | Val loss: 1.761 | Gen: ellway away ontionseray-away-awa insay-away-away-away onseredsay-away-away\n",
            "Epoch:   4 | Train loss: 1.539 | Val loss: 1.685 | Gen: ellway ayday ontionsestay insay orseredway\n",
            "Epoch:   5 | Train loss: 1.467 | Val loss: 1.619 | Gen: eredsay ayday ontionsentay insay orkersay\n",
            "Epoch:   6 | Train loss: 1.411 | Val loss: 1.568 | Gen: elway-inway-inway-in ay-inway ontionsestay-ondingw insay orkedhay\n",
            "Epoch:   7 | Train loss: 1.361 | Val loss: 1.538 | Gen: eway-awlay ay-inway onincionsay-ingentay insay orkferway\n",
            "Epoch:   8 | Train loss: 1.319 | Val loss: 1.498 | Gen: eationday ay-inway oninsinglessway insway orfinghay\n",
            "Epoch:   9 | Train loss: 1.278 | Val loss: 1.504 | Gen: estay ayday oningionday insway orfshay-awlay\n",
            "Epoch:  10 | Train loss: 1.262 | Val loss: 1.540 | Gen: eway ay-incay ontiondedway insay orfhedway\n",
            "Epoch:  11 | Train loss: 1.244 | Val loss: 1.490 | Gen: eway-ayday ayghay ontiondedway inssway orkfshay\n",
            "Epoch:  12 | Train loss: 1.216 | Val loss: 1.524 | Gen: eway aingway ontiondway insway orkfay-ondway\n",
            "Epoch:  13 | Train loss: 1.198 | Val loss: 1.473 | Gen: eationday aingway ontionday inssway orkfsay\n",
            "Epoch:  14 | Train loss: 1.168 | Val loss: 1.440 | Gen: eway-ayday ayghay ontiondsay-ondedway inssway orksondway\n",
            "Epoch:  15 | Train loss: 1.159 | Val loss: 1.500 | Gen: erhay aygay ondinglyway ingway orkingway\n",
            "Epoch:  16 | Train loss: 1.150 | Val loss: 1.443 | Gen: eway-ayday aifstway ondingingsay inssway orkingway\n",
            "Epoch:  17 | Train loss: 1.135 | Val loss: 1.435 | Gen: estray ay-iwedway ondingroustray-ayday ingway orkingway\n",
            "Epoch:  18 | Train loss: 1.118 | Val loss: 1.427 | Gen: erhay aifyway onintionday-ayday inssway orkingday\n",
            "Epoch:  19 | Train loss: 1.098 | Val loss: 1.436 | Gen: erhay aifsay onsingingway inssway orkingway\n",
            "Epoch:  20 | Train loss: 1.084 | Val loss: 1.439 | Gen: eshay aingway onsingingsay inssway orkingway\n",
            "Epoch:  21 | Train loss: 1.074 | Val loss: 1.397 | Gen: eshay aingway onsingingsay ingway orfondway-ayday\n",
            "Epoch:  22 | Train loss: 1.062 | Val loss: 1.432 | Gen: eshay aingway onsingingsay inssway orkingway\n",
            "Epoch:  23 | Train loss: 1.046 | Val loss: 1.432 | Gen: eshay aifsay ondingingday ingway orkingway\n",
            "Epoch:  24 | Train loss: 1.049 | Val loss: 1.546 | Gen: eshay aingway onintionsay-ayday inssway-ayday orkingsay-oowlay\n",
            "Epoch:  25 | Train loss: 1.053 | Val loss: 1.425 | Gen: eshay aingway ondingray-imedtay issway orfingsay\n",
            "Epoch:  26 | Train loss: 1.031 | Val loss: 1.454 | Gen: eshay aingway ondingingsay issway orkingsay\n",
            "Epoch:  27 | Train loss: 1.011 | Val loss: 1.433 | Gen: eshay aingway ondingingsay insway orkingway\n",
            "Epoch:  28 | Train loss: 1.000 | Val loss: 1.426 | Gen: eshay aingway ondingsray-imeday issway orfingsay\n",
            "Epoch:  29 | Train loss: 0.991 | Val loss: 1.455 | Gen: eshay aingway ondingingsay issway orkingsay\n",
            "Epoch:  30 | Train loss: 0.985 | Val loss: 1.441 | Gen: eshay aingway ondingingway issway orkingway\n",
            "Epoch:  31 | Train loss: 1.000 | Val loss: 1.492 | Gen: eshhay aingway-ayday ontionssay-ondedway- issway-ayday orfingsay-ayday\n",
            "Epoch:  32 | Train loss: 0.995 | Val loss: 1.426 | Gen: eshay aingway onditionssay-ayday issway orfonduray\n",
            "Epoch:  33 | Train loss: 0.975 | Val loss: 1.429 | Gen: eshhay aingway ondingingsray issway orkingway\n",
            "Epoch:  34 | Train loss: 0.966 | Val loss: 1.410 | Gen: ethay aingway ondingstray-awlay issway orfondway-andway-awl\n",
            "Epoch:  35 | Train loss: 0.963 | Val loss: 1.459 | Gen: eshhay aingway ondintionssay-ondway issway orkingsay\n",
            "Epoch:  36 | Train loss: 0.959 | Val loss: 1.442 | Gen: ethay aingway ondingray-impasenati issway orfinglay\n",
            "Epoch:  37 | Train loss: 0.959 | Val loss: 1.432 | Gen: eshedway aingway onintionssay-owhay-a issway orkingsay\n",
            "Epoch:  38 | Train loss: 0.931 | Val loss: 1.381 | Gen: ehay-awlay aingway ondingingsray issway orkingsay\n",
            "Epoch:  39 | Train loss: 0.917 | Val loss: 1.332 | Gen: ethay aingway onintionday-atforway issway orkingway\n",
            "Epoch:  40 | Train loss: 0.908 | Val loss: 1.343 | Gen: ethay aingway onintionshay issway orkingsay\n",
            "Epoch:  41 | Train loss: 0.904 | Val loss: 1.322 | Gen: ethay aingway onintionssay-ovedway issway orkingway\n",
            "Epoch:  42 | Train loss: 0.902 | Val loss: 1.387 | Gen: ethay ayghay onintionshay issway orkingsay\n",
            "Epoch:  43 | Train loss: 0.913 | Val loss: 1.363 | Gen: ethay aiftway-awlay onintionssay-ovedray issway orkingsway\n",
            "Epoch:  44 | Train loss: 0.907 | Val loss: 1.460 | Gen: ehhay-awlay ayghay onditionsshentay issway orkingsay-ofway\n",
            "Epoch:  45 | Train loss: 0.895 | Val loss: 1.332 | Gen: ethay aygray onintionsshay issway orkingway\n",
            "Epoch:  46 | Train loss: 0.881 | Val loss: 1.396 | Gen: ethay ayghay ondingingsray issway orkingway\n",
            "Epoch:  47 | Train loss: 0.882 | Val loss: 1.316 | Gen: ethay aingway onintionsshay-awlay issway orkingway\n",
            "Epoch:  48 | Train loss: 0.878 | Val loss: 1.450 | Gen: ehay-astway-awlay ayghay ondingingsray issway orfinglyway\n",
            "Epoch:  49 | Train loss: 0.881 | Val loss: 1.322 | Gen: ehay aygray onintionshedsay issway orkingway\n",
            "Epoch:  50 | Train loss: 0.864 | Val loss: 1.343 | Gen: ehay-ayday aygray ondingingsray issway orkfondway-ofway\n",
            "Epoch:  51 | Train loss: 0.856 | Val loss: 1.305 | Gen: ehay-awlay aygray onintionshedway issway orkfondway-away-awla\n",
            "Epoch:  52 | Train loss: 0.850 | Val loss: 1.369 | Gen: ethay ayghay ondingingedray issway orkfondway-ofway\n",
            "Epoch:  53 | Train loss: 0.848 | Val loss: 1.290 | Gen: ethay aingway onintionsshentway issway orkingway\n",
            "Epoch:  54 | Train loss: 0.847 | Val loss: 1.372 | Gen: ehay-awlay ayghay onditionsshay-awlay issway orkfunsgray\n",
            "Epoch:  55 | Train loss: 0.851 | Val loss: 1.303 | Gen: ethay aygray ondingingsray issway orkingway\n",
            "Epoch:  56 | Train loss: 0.850 | Val loss: 1.547 | Gen: ehay-awlay aygray onsitionentway-ayday issway orkingshay\n",
            "Epoch:  57 | Train loss: 0.858 | Val loss: 1.330 | Gen: ebhay ayghay onintionshay issway orkfonway-inway-ybay\n",
            "Epoch:  58 | Train loss: 0.836 | Val loss: 1.305 | Gen: ehay-awlay ayghay ondingingsray issway orkfondway-ofway\n",
            "Epoch:  59 | Train loss: 0.824 | Val loss: 1.297 | Gen: ehay ayghay onditionshedway issway orkfonday\n",
            "Epoch:  60 | Train loss: 0.814 | Val loss: 1.267 | Gen: ehay ayghay onditionshedway issway orkfonday\n",
            "Epoch:  61 | Train loss: 0.809 | Val loss: 1.334 | Gen: ethay ayghay ondingingsray issway orkingshay\n",
            "Epoch:  62 | Train loss: 0.821 | Val loss: 1.314 | Gen: ehay aygray onintionsshay issway orkingway\n",
            "Epoch:  63 | Train loss: 0.811 | Val loss: 1.289 | Gen: ethay ayghay ondingingsray issway orkfondway-inway-awl\n",
            "Epoch:  64 | Train loss: 0.807 | Val loss: 1.372 | Gen: ethay ayghay ondingingsray issway orkinghay\n",
            "Epoch:  65 | Train loss: 0.821 | Val loss: 1.370 | Gen: ehay ayghay onditionsedway issway-ybay orkfondway-ineway-aw\n",
            "Epoch:  66 | Train loss: 0.815 | Val loss: 1.295 | Gen: ethay ayghay onintionstay issway orkingsay\n",
            "Epoch:  67 | Train loss: 0.804 | Val loss: 1.307 | Gen: ethay aygray ondingsingsray issway orkfondway\n",
            "Epoch:  68 | Train loss: 0.799 | Val loss: 1.346 | Gen: ethay aybybay ondingsresshentway issway orkfonungway\n",
            "Epoch:  69 | Train loss: 0.828 | Val loss: 1.472 | Gen: ethay ayghay oningtentway issway orfrghfay\n",
            "Epoch:  70 | Train loss: 0.889 | Val loss: 1.353 | Gen: eheray ayghay ondingsorteway issway orkingway\n",
            "Epoch:  71 | Train loss: 0.821 | Val loss: 1.300 | Gen: ethay ayghay onditionsshay issway orkingway\n",
            "Epoch:  72 | Train loss: 0.786 | Val loss: 1.241 | Gen: ethay aybyray onditionshedway issway orkfondway\n",
            "Epoch:  73 | Train loss: 0.772 | Val loss: 1.237 | Gen: ethay ayghay onditionshedway issway orkfonfay\n",
            "Epoch:  74 | Train loss: 0.765 | Val loss: 1.223 | Gen: ethay aybybay onditionshedway issway orkfondway\n",
            "Epoch:  75 | Train loss: 0.759 | Val loss: 1.218 | Gen: ethay ayghay onditionshedway issway orkfondway\n",
            "Epoch:  76 | Train loss: 0.755 | Val loss: 1.233 | Gen: ethay aybybay onditionshedway issway orkfondway\n",
            "Epoch:  77 | Train loss: 0.754 | Val loss: 1.221 | Gen: ethay ayghay onditionshedway issway orkfondway\n",
            "Epoch:  78 | Train loss: 0.754 | Val loss: 1.301 | Gen: ethay aygray ondintionsshay issway orkfondway\n",
            "Epoch:  79 | Train loss: 0.768 | Val loss: 1.288 | Gen: ethay-inway-erberay ayghay onditionshedway issway orkingway\n",
            "Epoch:  80 | Train loss: 0.773 | Val loss: 1.292 | Gen: ehay-awlay aingway onditionsedway issay orkfonfurday\n",
            "Epoch:  81 | Train loss: 0.774 | Val loss: 1.264 | Gen: ethay aygray onditionshedway issway orkingway\n",
            "Epoch:  82 | Train loss: 0.758 | Val loss: 1.340 | Gen: ethay aygray ondingssreshay issway orkfonway-ybay\n",
            "Epoch:  83 | Train loss: 0.756 | Val loss: 1.282 | Gen: ethay aygray onditionshedway issway orkfondway-ofway\n",
            "Epoch:  84 | Train loss: 0.744 | Val loss: 1.279 | Gen: ethay aygray onditionshedway issway orkfonday\n",
            "Epoch:  85 | Train loss: 0.741 | Val loss: 1.263 | Gen: ethay aygray ondingsentway-inway- isway orkfondway-ybay\n",
            "Epoch:  86 | Train loss: 0.741 | Val loss: 1.267 | Gen: ethay aygray onditionshedway issway orkfondway\n",
            "Epoch:  87 | Train loss: 0.736 | Val loss: 1.300 | Gen: ethay aygray ondintionssay-inway- issay orkfonuway\n",
            "Epoch:  88 | Train loss: 0.746 | Val loss: 1.352 | Gen: ethay-owhay aygray onditionshay issway orkingway\n",
            "Epoch:  89 | Train loss: 0.758 | Val loss: 1.267 | Gen: ethay aygray ondintionssay issay orkfunsday\n",
            "Epoch:  90 | Train loss: 0.743 | Val loss: 1.297 | Gen: ethay aygray onditionshay issway orkfondway\n",
            "Epoch:  91 | Train loss: 0.749 | Val loss: 1.263 | Gen: ebhay-ardway aygray ondingsingsray issway orkingway\n",
            "Epoch:  92 | Train loss: 0.738 | Val loss: 1.237 | Gen: ethay aygray ondingsentshay issay orkfondway\n",
            "Epoch:  93 | Train loss: 0.730 | Val loss: 1.261 | Gen: ethay aygray ondingssresshay issay orkfonuway\n",
            "Epoch:  94 | Train loss: 0.725 | Val loss: 1.300 | Gen: ethay-awlay aygray onditionshedway issway orkingway\n",
            "Epoch:  95 | Train loss: 0.740 | Val loss: 1.236 | Gen: ebhay aygray onditionshedway issay orkfonuway\n",
            "Epoch:  96 | Train loss: 0.719 | Val loss: 1.277 | Gen: ebay-ovedway-ybay aybybay onditionsshay issay orkfondway\n",
            "Epoch:  97 | Train loss: 0.727 | Val loss: 1.214 | Gen: ethay-inway-awlay aygray ondingssershay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.712 | Val loss: 1.215 | Gen: ethay aygray ondingssresshay issay orkingway\n",
            "Epoch:  99 | Train loss: 0.705 | Val loss: 1.198 | Gen: ethay aygray ondingssresshay issay orkfonday\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay aygray ondingssresshay issay orkfonday\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE4ijaCzneAt",
        "colab_type": "text"
      },
      "source": [
        "Try translating different sentences by changing the variable TEST_SENTENCE. Identify two distinct failure modes and briefly describe them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrNnz8W1nULf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "688475f6-89ae-44df-8114-ab377f992c97"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay aygray ondingssresshay issay orkfonday\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWwA6OGqlaTq",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Additive Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJSafHSAmu_w",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Additive Attention\n",
        "Already implemented the additive attention mechanism. Write down the mathematical expression for $\\tilde{\\alpha}_i^{(t)}, \\alpha_i^{(t)}, c_t$ as a function of $W_1, W_2, b_1, b_2, Q_t, K_i$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdewEVSMo5jJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "                                    nn.Linear(hidden_size*2, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(hidden_size, 1)\n",
        "                                 )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "        batch_size = keys.size(0)\n",
        "        expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(keys)\n",
        "        concat_inputs = torch.cat([expanded_queries, keys], dim=2)\n",
        "        unnormalized_attention = self.attention_network(concat_inputs)\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights.transpose(2,1), values)\n",
        "        return context, attention_weights\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73_p8d5EmvOJ",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: RNN Additive Attention Decoder\n",
        "We will now implement a recurrent decoder that makes use of the additive attention mechanism. Read the description in the assignment worksheet and complete the following implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJaABkXrpJSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
        "        if attention_type == 'additive':\n",
        "          self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == 'scaled_dot':\n",
        "          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            # ------------\n",
        "            # FILL THIS IN - START\n",
        "            # ------------\n",
        "            embed_current = embed[:,i,:]  # Get the current time step, across the whole batch\n",
        "            context, attention_weights = self.attention(\n",
        "                h_prev, # queries @ (bs, hidden_size)\n",
        "                annotations, # keys @ (bs, sl, hs)\n",
        "                annotations # values @ (bs, sl, hs)\n",
        "            )  # @ (batch_size, 1,  hidden_size) and (batch_size, seq_len, 1)\n",
        "            embed_and_context = torch.cat((\n",
        "                embed_current.view(batch_size, -1),\n",
        "                context.view(batch_size, -1)),\n",
        "                dim=1\n",
        "            )  # batch_size x (2*hidden_size) \n",
        "            h_prev = self.rnn(embed_and_context, h_prev)  # batch_size x hidden_size \n",
        "            # ------------\n",
        "            # FILL THIS IN - END\n",
        "            # ------------    \n",
        "            \n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYPae08Io1Fi",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "Train the following language model that uses a recurrent encoder, and a recurrent decoder that has an additive attention component. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3-FuzY1pepu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "96108543-5d3b-4d56-f20b-f554302ffbf9"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'additive',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_attn_encoder, rnn_attn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 20                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn_attention                          \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('contrived', 'ontrivedcay')\n",
            "('gilberts', 'ilbertsgay')\n",
            "('agitated', 'agitatedway')\n",
            "('strictest', 'icteststray')\n",
            "('gaining', 'aininggay')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.213 | Val loss: 1.976 | Gen: otestay-intay-intay- away ontinsay onsay onsay-insay-insay-in\n",
            "Epoch:   1 | Train loss: 1.731 | Val loss: 1.817 | Gen: othay-y-othay-y-otha away ondingday-ingway insay-issay-issay-is onsingsay-ingsay-ing\n",
            "Epoch:   2 | Train loss: 1.536 | Val loss: 1.646 | Gen: othay away onsingsay issay-issay orsingshay\n",
            "Epoch:   3 | Train loss: 1.362 | Val loss: 1.587 | Gen: othtay away ondgay-ingway issay-issay-issay-is orshingghay\n",
            "Epoch:   4 | Train loss: 1.206 | Val loss: 1.416 | Gen: etay away ondingway issay orrongingway\n",
            "Epoch:   5 | Train loss: 1.068 | Val loss: 1.375 | Gen: eftay away ondingingingway issirray orshinginggay\n",
            "Epoch:   6 | Train loss: 0.916 | Val loss: 1.320 | Gen: eththay away onditiondingingingay issay orsingday\n",
            "Epoch:   7 | Train loss: 0.806 | Val loss: 1.254 | Gen: etay airyway onditingingingway issisay orlingway\n",
            "Epoch:   8 | Train loss: 0.686 | Val loss: 1.064 | Gen: etay aiway onditiongiongiongay- ispy-y-iway orliorway\n",
            "Epoch:   9 | Train loss: 0.575 | Val loss: 1.130 | Gen: ethay arway onditingay-ongway ispy orkingway\n",
            "Epoch:  10 | Train loss: 0.531 | Val loss: 0.865 | Gen: etay airway onditiongingingingay isway orkingway\n",
            "Epoch:  11 | Train loss: 0.441 | Val loss: 0.747 | Gen: ethay ariway ondititiongway isway orkinglway\n",
            "Epoch:  12 | Train loss: 0.356 | Val loss: 0.738 | Gen: etay arway onditioningway isway orkingway\n",
            "Epoch:  13 | Train loss: 0.341 | Val loss: 0.877 | Gen: ethay arrway onditiongcay isway orkingway\n",
            "Epoch:  14 | Train loss: 0.283 | Val loss: 0.623 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  15 | Train loss: 0.270 | Val loss: 1.147 | Gen: ethay ay-reway onditioniongnay-oong isay orlingnyway\n",
            "Epoch:  16 | Train loss: 0.391 | Val loss: 0.665 | Gen: ethay array onditiongicgay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.219 | Val loss: 0.496 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  18 | Train loss: 0.185 | Val loss: 0.527 | Gen: ethay airway onditiongingcay isway orkingimrway\n",
            "Epoch:  19 | Train loss: 0.144 | Val loss: 0.484 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.128 | Val loss: 0.478 | Gen: ethay airway onditioingay-ingcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.128 | Val loss: 0.440 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.098 | Val loss: 0.492 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  23 | Train loss: 0.116 | Val loss: 0.476 | Gen: ethhay airway onditioningcay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.102 | Val loss: 0.955 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.113 | Val loss: 0.753 | Gen: ethay airray ondititioningcay isay-y-iray orkingimgway\n",
            "Epoch:  26 | Train loss: 0.168 | Val loss: 1.187 | Gen: ethay arway onditiongcay isway orkingray\n",
            "Epoch:  27 | Train loss: 0.171 | Val loss: 0.459 | Gen: ethay airway onditiongay-ingcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.097 | Val loss: 0.575 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.090 | Val loss: 0.452 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.071 | Val loss: 1.370 | Gen: ethay airway onditioiningay isway orkingway-y-ongway\n",
            "Epoch:  31 | Train loss: 0.114 | Val loss: 0.405 | Gen: ethay airway onditiongay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.062 | Val loss: 0.317 | Gen: ethay airway onditiongay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.046 | Val loss: 0.382 | Gen: ethay airay onditioningcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.054 | Val loss: 0.426 | Gen: ethay airway ondiitiongcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.092 | Val loss: 0.405 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.053 | Val loss: 0.310 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.028 | Val loss: 0.316 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.033 | Val loss: 0.330 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.030 | Val loss: 0.318 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.023 | Val loss: 0.302 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.019 | Val loss: 0.316 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.017 | Val loss: 0.287 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.012 | Val loss: 0.275 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.011 | Val loss: 0.279 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.009 | Val loss: 0.282 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.008 | Val loss: 0.283 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.007 | Val loss: 0.277 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.006 | Val loss: 0.293 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.006 | Val loss: 0.287 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  50 | Train loss: 0.022 | Val loss: 0.987 | Gen: ethay airray ondiitiongcay isway orkingway\n",
            "Epoch:  51 | Train loss: 0.264 | Val loss: 0.804 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  52 | Train loss: 0.196 | Val loss: 0.610 | Gen: ethay airway onditiongay isway orkingway\n",
            "Epoch:  53 | Train loss: 0.153 | Val loss: 0.412 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  54 | Train loss: 0.057 | Val loss: 0.313 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.028 | Val loss: 0.288 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  56 | Train loss: 0.019 | Val loss: 0.288 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.014 | Val loss: 0.284 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  58 | Train loss: 0.011 | Val loss: 0.279 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.010 | Val loss: 0.278 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  60 | Train loss: 0.009 | Val loss: 0.278 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.008 | Val loss: 0.273 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  62 | Train loss: 0.007 | Val loss: 0.274 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.006 | Val loss: 0.273 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.006 | Val loss: 0.274 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.005 | Val loss: 0.274 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.005 | Val loss: 0.273 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.004 | Val loss: 0.274 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.004 | Val loss: 0.275 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  69 | Train loss: 0.004 | Val loss: 0.276 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.003 | Val loss: 0.274 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.003 | Val loss: 0.276 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.003 | Val loss: 0.277 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.003 | Val loss: 0.277 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.003 | Val loss: 0.277 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.002 | Val loss: 0.278 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.002 | Val loss: 0.278 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.002 | Val loss: 0.279 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.002 | Val loss: 0.279 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.002 | Val loss: 0.279 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.002 | Val loss: 0.278 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.002 | Val loss: 0.280 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.002 | Val loss: 0.279 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.001 | Val loss: 0.279 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.001 | Val loss: 0.279 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.001 | Val loss: 0.279 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.001 | Val loss: 0.278 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.001 | Val loss: 0.279 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.001 | Val loss: 0.277 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.001 | Val loss: 0.278 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.001 | Val loss: 0.276 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.001 | Val loss: 0.279 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.133 | Val loss: 0.837 | Gen: ehay airay ondiitiongnycay isay orkingway\n",
            "Epoch:  93 | Train loss: 0.178 | Val loss: 0.616 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.058 | Val loss: 0.258 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.016 | Val loss: 0.227 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.010 | Val loss: 0.217 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.007 | Val loss: 0.214 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.005 | Val loss: 0.221 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.004 | Val loss: 0.223 | Gen: ethay airway onditioningcay isway orkingway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNVKbLc0ACj_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "cd6070a5-73d6-4012-cc82-50135ea2b587"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw_GOIvzo1ix",
        "colab_type": "text"
      },
      "source": [
        "# Part 3: Scaled Dot Product Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq7nhsEio1w-",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Implement Dot-Product Attention\n",
        "Implement the scaled dot product attention module described in the assignment worksheet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_j3oY3hqsJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        hidden_size = self.hidden_size\n",
        "        batch_size = queries.shape[0]\n",
        "        d = hidden_size\n",
        "        # Convert tensor to 3D.\n",
        "        # k is the number of queries.\n",
        "        queries = queries.view(batch_size, -1, hidden_size)\n",
        "        num_queries = queries.shape[1]\n",
        "        seq_len = keys.shape[1]\n",
        "        # Expand.\n",
        "        # keys = keys.expand(batch_size, seq_len, hidden_size)\n",
        "        # keys = torch.transpose(keys, dim0=0, dim1=1)\n",
        "\n",
        "        q = self.Q(queries) # @ (batch_size, k, hidden_size)\n",
        "        k = self.K(keys) # @ (batch_size, seq_len, hidden_size)\n",
        "        v = self.V(values) # @ (batch_size, seq_len, hidden_size)\n",
        "        q = torch.transpose(q, 1, 2) # @ (batch_size, hidden_size, k)\n",
        "        # print(\"q @\", q.shape)\n",
        "        # print(\"k @\", k.shape)\n",
        "        unnormalized_attention = torch.bmm(k, q) * self.scaling_factor\n",
        "        # unnormalized_attention @ (batch_size, seq_len, k)\n",
        "        # print(unnormalized_attention.shape)\n",
        "    \n",
        "        attention_weights = self.softmax(unnormalized_attention).transpose(1, 2) # @ (batch_size, k, seq_len)\n",
        "        context = torch.bmm(attention_weights, v) # @ (batch_size, k, hidden_size)\n",
        "        return context, attention_weights\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA8oTAPqyv8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test code.\n",
        "att = ScaledDotAttention(20)\n",
        "annot = torch.randn(64, 10, 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EcH2Luny8Zh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c, a = att(annot, annot, annot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unReAOrjo113",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Implement Causal Dot-Product Attention\n",
        "Now implement the scaled causal dot product described in the assignment worksheet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovigzQffrKqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        hidden_size = self.hidden_size\n",
        "        batch_size = queries.shape[0]\n",
        "        d = hidden_size\n",
        "        # Convert tensor to 3D.\n",
        "        # k is the number of queries.\n",
        "        queries = queries.view(batch_size, -1, hidden_size)\n",
        "        num_queries = queries.shape[1]\n",
        "        seq_len = keys.shape[1]\n",
        "        # keys = keys.expand(batch_size, seq_len, hidden_size)\n",
        "        # keys = torch.transpose(keys, dim0=0, dim1=1)\n",
        "\n",
        "        q = self.Q(queries) # @ (batch_size, k, hidden_size)\n",
        "        k = self.K(keys) # @ (batch_size, seq_len, hidden_size)\n",
        "        v = self.V(values) # @ (batch_size, seq_len, hidden_size)\n",
        "        q = torch.transpose(q, 2, 1) # @ (batch_size, hidden_size, k)\n",
        "        # print(\"q @\", q.shape)\n",
        "        # print(\"k @\", k.shape)\n",
        "        unnormalized_attention = torch.bmm(k, q) * self.scaling_factor\n",
        "        # unnormalized_attention @ (batch_size, seq_len, k)\n",
        "        # print(unnormalized_attention.shape)\n",
        "        # ==== Enforce Casual ====\n",
        "        # Mask looks like\n",
        "        \"\"\"\n",
        "        tensor([[0., 1., 1., 1.],\n",
        "                [0., 0., 1., 1.],\n",
        "                [0., 0., 0., 1.]]) * -inf.\n",
        "        \"\"\"\n",
        "        mask = torch.tril(torch.ones_like(unnormalized_attention)) * self.neg_inf\n",
        "        unnormalized_attention += mask\n",
        "        # ==== End ====\n",
        "        attention_weights = self.softmax(unnormalized_attention).transpose(1, 2) # @ (batch_size, k, seq_len)\n",
        "        context = torch.bmm(attention_weights, v) # @ (batch_size, k, hidden_size)\n",
        "        return context, attention_weights\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tcpUFKqo2Oi",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Transformer Encoder\n",
        "Complete the following transformer encoder implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3B-fWsarlVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        \n",
        "        # IMPORTANT CORRECTION: NON-CAUSAL ATTENTION SHOULD HAVE BEEN\n",
        "        # USED IN THE TRANSFORMER ENCODER. \n",
        "        # NEW VERSION: \n",
        "        self.self_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        # PREVIONS VERSION: \n",
        "        # self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
        "        #                             hidden_size=hidden_size, \n",
        "        #                          ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        # ------------\n",
        "        # FILL THIS IN - START\n",
        "        # ------------\n",
        "        encoded = self.embedding(inputs)  # @ (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n",
        "        # self.positional_encodings @ (max_seq_len, hidden_size)\n",
        "        # Get the first few positional encodings.\n",
        "        # pos = self.positional_encodings[:seq_len, :] # @ (seq_len, hidden_size)\n",
        "        # Expand to batches.\n",
        "        # pos = pos.expand(batch_size, seq_len, self.hidden_size)\n",
        "        encoded = encoded + self.positional_encodings[:seq_len]\n",
        "\n",
        "        # Initiate hidden states, the first hidden state is the embedded input layer.\n",
        "        annotations = encoded # @ (batch_size, seq_len, hidden_size)\n",
        "        # ==== Shapes ====\n",
        "        # print(inputs.shape) # 64, 10\n",
        "        # print(self.embedding(inputs).shape) # 64, 10, 20\n",
        "        # print(annotations.shape) # 64, 10, 20\n",
        "        # ==== End ====\n",
        "        for i in range(self.num_layers):\n",
        "            new_annotations, self_attention_weights = self.self_attentions[i](\n",
        "                annotations, annotations, annotations\n",
        "            )  # batch_size x seq_len x hidden_size\n",
        "            # annotation with residual added.\n",
        "            residual_annotations = annotations + new_annotations\n",
        "            new_annotations = self.attention_mlps[i](residual_annotations)\n",
        "            # Update annotations, the output of this layer.\n",
        "            annotations = residual_annotations + new_annotations\n",
        "        # ------------\n",
        "        # FILL THIS IN - END\n",
        "        # ------------\n",
        "\n",
        "        # Transformer encoder does not have a last hidden layer. \n",
        "        return annotations, None  \n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "      \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "      Arguments:\n",
        "          max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "      Returns:\n",
        "          pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
        "      \"\"\"\n",
        "      pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "      dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
        "      exponents = (2*dim_indices).float()/(self.hidden_size)\n",
        "      trig_args = pos_indices / (10000**exponents)\n",
        "      sin_terms = torch.sin(trig_args)\n",
        "      cos_terms = torch.cos(trig_args)\n",
        "\n",
        "      pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "      pos_encodings[:, 0::2] = sin_terms\n",
        "      pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "      if self.opts.cuda:\n",
        "        pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "      return pos_encodings\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1hDi020rT36",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Transformer Decoder\n",
        "Complete the following transformer decoder implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyvTZFxtrvc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size \n",
        "\n",
        "        # THIS LINE WAS ADDED AS A CORRECTION. \n",
        "        embed = embed + self.positional_encodings[:seq_len]       \n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "\n",
        "        # Decoder: the input fed to the first layer.\n",
        "        contexts = embed # batch_size x seq_len x hidden_size \n",
        "        for i in range(self.num_layers):\n",
        "            # ------------\n",
        "            # FILL THIS IN - START\n",
        "            # ------------\n",
        "            new_contexts, self_attention_weights = self.self_attentions[i](\n",
        "                contexts, contexts, contexts\n",
        "            ) # batch_size x seq_len x hidden_size\n",
        "            # print(annotations.shape)\n",
        "            # print(new_contexts.shape)\n",
        "            # print(contexts.shape)\n",
        "            residual_contexts = contexts + new_contexts\n",
        "            new_contexts, encoder_attention_weights = self.encoder_attentions[i](\n",
        "                residual_contexts, annotations, annotations\n",
        "            ) # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = residual_contexts + new_contexts\n",
        "            new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "            contexts = residual_contexts + new_contexts\n",
        "\n",
        "            # ------------\n",
        "            # FILL THIS IN - END\n",
        "            # ------------\n",
        "          \n",
        "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "            self_attention_weights_list.append(self_attention_weights)\n",
        "          \n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "        \n",
        "        return output, (encoder_attention_weights, self_attention_weights)\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "      \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "      Arguments:\n",
        "          max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "      Returns:\n",
        "          pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
        "      \"\"\"\n",
        "      pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "      dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
        "      exponents = (2*dim_indices).float()/(self.hidden_size)\n",
        "      trig_args = pos_indices / (10000**exponents)\n",
        "      sin_terms = torch.sin(trig_args)\n",
        "      cos_terms = torch.cos(trig_args)\n",
        "\n",
        "      pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "      pos_encodings[:, 0::2] = sin_terms\n",
        "      pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "      pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "      return pos_encodings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ZjkXTNrUKb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Step 5: Training and analysis\n",
        "Now, train the following language model that's comprised of a (simplified) transformer encoder and transformer decoder. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmoTgrDcr_dw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "outputId": "ceb7c423-1293-4526-bb5b-267aa2f765c2"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005,  ## INCREASE BY AN ORDER OF MAGNITUDE\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "transformer_encoder, transformer_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 20                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('contrived', 'ontrivedcay')\n",
            "('gilberts', 'ilbertsgay')\n",
            "('agitated', 'agitatedway')\n",
            "('strictest', 'icteststray')\n",
            "('gaining', 'aininggay')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.201 | Val loss: 2.054 | Gen: etrtttteeety oarayEOSyiy ondayEOSnoy onsoooonny ortiiorany\n",
            "Epoch:   1 | Train loss: 1.645 | Val loss: 1.907 | Gen: eway-etewey eway-yyy oncoionnntcay ossasasay omay-y\n",
            "Epoch:   2 | Train loss: 1.422 | Val loss: 1.856 | Gen: eway-ehehehay ariaayy oootiioioioiooooooio ay-ioiiiaiiaay oray-iifm-EOSEOSnnaEOSy\n",
            "Epoch:   3 | Train loss: 1.264 | Val loss: 1.731 | Gen: eataayyy aayay incoooooiiayyonny aay iwwwewnwaay\n",
            "Epoch:   4 | Train loss: 1.049 | Val loss: 1.596 | Gen: eearyraayEOSEOSEOSeeey arieray ioictninnnaay ywaywaywaywaywywaayw ilwaiyyy\n",
            "Epoch:   5 | Train loss: 0.957 | Val loss: 1.575 | Gen: ewwewwway iwaayEOSyEOSttwaayy indondinday ywowwwwwywywywywwywy orwoaawayy\n",
            "Epoch:   6 | Train loss: 0.894 | Val loss: 1.554 | Gen: ebtttwwway ittriatbtttiiEOSaay intitttnnngnggaagggy ibbbbbbbbbbbwwwwwiia ettngriay\n",
            "Epoch:   7 | Train loss: 0.821 | Val loss: 1.275 | Gen: etthay aa-ay ondititinginaoyyonnn ayaasy otingray-aayy\n",
            "Epoch:   8 | Train loss: 0.656 | Val loss: 1.377 | Gen: ettwrray aaayy intttottnnEOSEOSgaaay yssismaay orkingray\n",
            "Exiting early from training.\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehaay iirrwaayaay onditingdingay iswwanwaay iayaay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R18s80gzC6A8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "902fa866-b32a-452b-a09b-0e56896caac6"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcaaEOSy isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBnBXRG8mvcn",
        "colab_type": "text"
      },
      "source": [
        "# Optional: Attention Visualizations\n",
        "\n",
        "One of the benefits of using attention is that it allows us to gain insight into the inner workings of the model.\n",
        "\n",
        "By visualizing the attention weights generated for the input tokens in each decoder step, we can see where the model focuses while producing each output token.\n",
        "\n",
        "The code in this section loads the model you trained from the previous section and uses it to translate a given set of words: it prints the translations and display heatmaps to show how attention is used at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqEC0vN9mvpV",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Visualize Attention Masks\n",
        "Play around with visualizing attention maps generated by the previous two models you've trained. Inspect visualizations in one success and one failure case for both models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkfz-u-MtudL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "f3dc862b-1771-47b7-907b-dd2941ca2729"
      },
      "source": [
        "TEST_WORD_ATTN = 'street'\n",
        "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEhCAYAAABRKfYcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npogG6ZJKVwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eetstray'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ssa7g35zt2yj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "outputId": "2a90e8b0-6c30-4cf2-eb2f-8e912485e63e"
      },
      "source": [
        "TEST_WORD_ATTN = 'street'\n",
        "visualize_attention(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args, )"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAASEAAAEYCAYAAAATaEB+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAASEAAAEYCAYAAAATaEB+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH2Kl5VQe6F9kAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAASEAAAEYCAYAAAATaEB+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSESiUhASkaj+P6Isg2LSz5UeAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eetstray'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owstslMF-wdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}