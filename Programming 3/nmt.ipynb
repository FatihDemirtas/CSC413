{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjPTaRB4mpCd",
        "colab_type": "text"
      },
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignmentby selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9IS9B9-yUU5",
        "colab_type": "text"
      },
      "source": [
        "## Setup PyTorch\n",
        "All files are stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axbuunY8UdTB",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-6MQhMOlHXD",
        "colab_type": "code",
        "outputId": "f876373c-0ded-4a19-9c62-c7a80c2db9e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install torch torchvision\n",
        "!pip install Pillow==4.0.0\n",
        "%mkdir -p /content/csc421/a3/\n",
        "%cd /content/csc421/a3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.5)\n",
            "Collecting pillow>=4.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/5e/23dcc0ce3cc2abe92efd3cd61d764bee6ccdf1b667a1fb566f45dc249953/Pillow-7.0.0-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pillow\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-7.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting Pillow==4.0.0\n",
            "  Using cached https://files.pythonhosted.org/packages/37/e8/b3fbf87b0188d22246678f8cd61e23e31caa1769ebc06f1664e2e5fe8a17/Pillow-4.0.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow==4.0.0) (0.46)\n",
            "\u001b[31mERROR: torchvision 0.5.0 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: scikit-image 0.16.2 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "Successfully installed Pillow-4.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content/csc421/a3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DaTdRNuUra7",
        "colab_type": "text"
      },
      "source": [
        "# Helper code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIpGwANoQOg",
        "colab_type": "text"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-UJHBYZkh7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "    \n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "        \n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel('Epochs', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
        "        pkl.dump(idx_dict, f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MglI7o8xs1Pk",
        "colab_type": "code",
        "outputId": "384eddc6-0976-42ed-e510-de66344ace97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.get_device_name()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbvpn4MaV0I1",
        "colab_type": "text"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVT4TNTOV3Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\n",
        "    \"\"\"\n",
        "    lines = open(filename).read().strip().lower().split('\\n')\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n",
        "    \"\"\"\n",
        "    return all(c.isalpha() or c == '-' for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n",
        "    \"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n",
        "    \"\"\"\n",
        "\n",
        "    source_lines, target_lines = read_pairs('data/pig_latin_data.txt')\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index['SOS'] = start_token\n",
        "    char_to_index['EOS'] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = { index: char for (char, index) in char_to_index.items() }\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = { 'char_to_index': char_to_index,\n",
        "                 'index_to_char': index_to_char,\n",
        "                 'start_token': start_token,\n",
        "                 'end_token': end_token }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s,t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s,t))\n",
        "\n",
        "    return d\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRWfRdmVVjUl",
        "colab_type": "text"
      },
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa5-onJhoSeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\n",
        "    \"\"\"\n",
        "    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\n",
        "    \"\"\"\n",
        "\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "\n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "      \n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "    \n",
        "    if isinstance(attention_weights, tuple):\n",
        "      ## transformer's attention mweights\n",
        "      attention_weights, self_attention_weights = attention_weights\n",
        "    \n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "    \n",
        "    for i in range(len(all_attention_weights)):\n",
        "      attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "      fig = plt.figure()\n",
        "      ax = fig.add_subplot(111)\n",
        "      cax = ax.matshow(attention_weights_matrix, cmap='bone')\n",
        "      fig.colorbar(cax)\n",
        "\n",
        "      # Set up axes\n",
        "      ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n",
        "      ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n",
        "\n",
        "      # Show label at every tick\n",
        "      ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      # Add title\n",
        "      plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n",
        "      plt.tight_layout()\n",
        "      plt.grid('off')\n",
        "      plt.show()\n",
        "      #plt.savefig(save)\n",
        "\n",
        "      #plt.close(fig)\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
        "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n",
        "            \n",
        "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, encoder_hidden)\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "              # Zero gradients\n",
        "              optimizer.zero_grad()\n",
        "              # Compute gradients\n",
        "              loss.backward()\n",
        "              # Update the parameters of the encoder and decoder\n",
        "              optimizer.step()\n",
        "              \n",
        "    mean_loss = np.mean(losses)\n",
        "    return mean_loss\n",
        "\n",
        "  \n",
        "\n",
        "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
        "        \n",
        "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
        "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, train_loss, val_loss, gen_string))\n",
        "\n",
        "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        save_loss_plot(train_losses, val_losses, opts)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Data Stats'.center(80))\n",
        "    print('-' * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print('Num unique word pairs: {}'.format(len(line_pairs)))\n",
        "    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n",
        "    print('Vocab size: {}'.format(vocab_size))\n",
        "    print('=' * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data()\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    if opts.encoder_type == \"rnn\":\n",
        "      encoder = GRUEncoder(vocab_size=vocab_size, \n",
        "                          hidden_size=opts.hidden_size, \n",
        "                          opts=opts)\n",
        "    elif opts.encoder_type == \"transformer\":\n",
        "      encoder = TransformerEncoder(vocab_size=vocab_size, \n",
        "                                   hidden_size=opts.hidden_size, \n",
        "                                   num_layers=opts.num_transformer_layers,\n",
        "                                   opts=opts)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if opts.decoder_type == 'rnn':\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, \n",
        "                             hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == 'rnn_attention':\n",
        "        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n",
        "                                      hidden_size=opts.hidden_size, \n",
        "                                      attention_type=opts.attention_type)\n",
        "    elif opts.decoder_type == 'transformer':\n",
        "        decoder = TransformerDecoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    #### setup checkpoint path\n",
        "    model_name = 'h{}-bs{}-{}'.format(opts.hidden_size, \n",
        "                                      opts.batch_size, \n",
        "                                      opts.decoder_type)\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n",
        "\n",
        "    try:\n",
        "        training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts)\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return encoder, decoder\n",
        "      \n",
        "    return encoder, decoder\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yh08KhgnA30",
        "colab_type": "text"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aROU2xZanDKq",
        "colab_type": "code",
        "outputId": "ecbcc021-a4ae-488a-b956-3f1726986438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(fname='pig_latin_data.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_data.txt', \n",
        "                         untar=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/pig_latin_data.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDYMr7NclZdw",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Gated Recurrent Unit (GRU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCae1mOUlZrC",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: GRU Cell\n",
        "Please implement the Gated Recurent Unit class defined in the next cell. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HMO7FD6l5RU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyGRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyGRUCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        ## Input linear layers\n",
        "        self.Wiz = nn.Linear(input_size, hidden_size, bias=False)\n",
        "        self.Wir = nn.Linear(input_size, hidden_size, bias=False)\n",
        "        self.Win = nn.Linear(input_size, hidden_size, bias=False)\n",
        "\n",
        "        ## Hidden linear layers\n",
        "        self.Whz = nn.Linear(hidden_size, hidden_size, bias=True)\n",
        "        self.Whr = nn.Linear(hidden_size, hidden_size, bias=True)\n",
        "        self.Whn = nn.Linear(hidden_size, hidden_size, bias=True)\n",
        "        \n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"Forward pass of the GRU computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        r = torch.sigmoid(self.Wir(x) + self.Whr(h_prev))\n",
        "        z = torch.sigmoid(self.Wiz(x) + self.Whz(h_prev))\n",
        "        g = torch.tanh(self.Win(x) + r * self.Whn(h_prev))\n",
        "        h_new = (1 - z) * g + z * h_prev\n",
        "        return h_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecEq4TP2lZ4Z",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: GRU Encoder\n",
        "Please inspect the following recurrent encoder/decoder implementations. Make sure to run the cells before proceeding. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jDNim2fmVJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GRUEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(GRUEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = MyGRUCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n",
        "            hidden = self.gru(x, hidden)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvwizYM9ma4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None        \n",
        "        \"\"\"        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSDTbsydlaGI",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "Train the following language model comprised of recurrent encoder and decoders. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51HladZJtG7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3YLrAjsmx_W",
        "colab_type": "code",
        "outputId": "563df573-ac34-4488-be09-bfed5e681d06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_encoder, rnn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 20                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('heirs', 'eirshay')\n",
            "('acquaintances', 'acquaintancesway')\n",
            "('uproar', 'uproarway')\n",
            "('mercy', 'ercymay')\n",
            "('indulgent', 'indulgentway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.351 | Val loss: 2.088 | Gen: ay ay ongway ay ongway\n",
            "Epoch:   1 | Train loss: 1.932 | Val loss: 1.926 | Gen: ay-ay-ay-ay-ay-ay-ay ay-ay-ay-ay-ay-ay-ay ontingway onway ongway\n",
            "Epoch:   2 | Train loss: 1.791 | Val loss: 1.832 | Gen: ay-ay-ay-ay-ay-ay-ay ay-ay-ay-ay-ay-ay-ay ongay-oray-oray-ay-a ingway ongway\n",
            "Epoch:   3 | Train loss: 1.686 | Val loss: 1.735 | Gen: eay-ay-ay-ay-ay-ay-a away ongay-orsay-oray-ora ingay-ay-ay-ay-ay-ay ongay-ingway\n",
            "Epoch:   4 | Train loss: 1.586 | Val loss: 1.700 | Gen: eationway away-away onsiongay-ay-ingway insay-ay-ay-ay-ay-ay ongay-ingway\n",
            "Epoch:   5 | Train loss: 1.519 | Val loss: 1.633 | Gen: eaway away-away onsiongay-oray-ay-ay issay ongay-oray-oray-ay-a\n",
            "Epoch:   6 | Train loss: 1.466 | Val loss: 1.607 | Gen: eationdway away-awlay onsiongay-ay-ingway inshay oongay-onday-ay-ay-a\n",
            "Epoch:   7 | Train loss: 1.433 | Val loss: 1.599 | Gen: eatway away-ineray-away onstay-onsay-onday-o issay oongay-ingay-away\n",
            "Epoch:   8 | Train loss: 1.404 | Val loss: 1.581 | Gen: eatway away-away ongshiongay-ay-ayway issay oongay-onday-ayway\n",
            "Epoch:   9 | Train loss: 1.369 | Val loss: 1.553 | Gen: eay-away ay-ationway ontionshingway issay oodgay-onday-ay-ay-a\n",
            "Epoch:  10 | Train loss: 1.346 | Val loss: 1.576 | Gen: etay aysay-awlay onsay-oonsay-onday-a issay-antersay oongay-oontay-awlay\n",
            "Epoch:  11 | Train loss: 1.326 | Val loss: 1.506 | Gen: elay away-awlay onssay-oosshay-oday issay onglysay\n",
            "Epoch:  12 | Train loss: 1.283 | Val loss: 1.489 | Gen: etay-ay-ay-ay-ay-ay- away-ay-ay-ay-ay-ay- onshingshay-ay-ay-ay issay onglyday\n",
            "Epoch:  13 | Train loss: 1.253 | Val loss: 1.453 | Gen: etay away-awlay onshingshay-inway-aw issay onglysay\n",
            "Epoch:  14 | Train loss: 1.230 | Val loss: 1.443 | Gen: etway away-awlay onsay-oosshay-inway- istay onglyday\n",
            "Epoch:  15 | Train loss: 1.231 | Val loss: 1.513 | Gen: ethay-awlay aysay oossiontay istay oodgay-oomsay-andent\n",
            "Epoch:  16 | Train loss: 1.229 | Val loss: 1.506 | Gen: ethay-awlay aysay onsiongay-oodshay-in issay ongorfay-orway-omday\n",
            "Epoch:  17 | Train loss: 1.191 | Val loss: 1.381 | Gen: etay-awlay aysay onshinglysay issway onglysray-omationday\n",
            "Epoch:  18 | Train loss: 1.163 | Val loss: 1.414 | Gen: ethay aysay onsionsay-onday-ay-a istay ongray-oday-ableway\n",
            "Epoch:  19 | Train loss: 1.149 | Val loss: 1.389 | Gen: etway aysay onsiongray-orday-ord istay oungday-ordsay-onday\n",
            "Epoch:  20 | Train loss: 1.138 | Val loss: 1.478 | Gen: ethay astsay ontonsday-onday-ayda issay onglyday\n",
            "Epoch:  21 | Train loss: 1.141 | Val loss: 1.393 | Gen: ethay aysay-ablestay ongorshionsway issway onglyfay-inway-anded\n",
            "Epoch:  22 | Train loss: 1.124 | Val loss: 1.450 | Gen: ethay aysay-ableway-away-a onsionshay-inway-and issway ongray-orway-omation\n",
            "Epoch:  23 | Train loss: 1.112 | Val loss: 1.379 | Gen: ethay aysay-ablestay ontousionsday issway ongray-ordiondway\n",
            "Epoch:  24 | Train loss: 1.089 | Val loss: 1.402 | Gen: ethay astsday onsionshiongray issway onglyfay\n",
            "Epoch:  25 | Train loss: 1.068 | Val loss: 1.331 | Gen: etay aysay-ableway ontoringshingsway issway ongray-orday-onway-a\n",
            "Epoch:  26 | Train loss: 1.054 | Val loss: 1.382 | Gen: ethay asstay onsiongray-orionsway issway ongray-orionday\n",
            "Epoch:  27 | Train loss: 1.048 | Val loss: 1.350 | Gen: etay asstay ononshringray-orway- issway ongorfay-orway-omati\n",
            "Epoch:  28 | Train loss: 1.055 | Val loss: 1.369 | Gen: ethay ayssay ontonsday-inway-onwa issway onglyday-ommatway\n",
            "Epoch:  29 | Train loss: 1.038 | Val loss: 1.350 | Gen: ethay aysay-ay-oray-away-a ontondsshay-inway-an issway ongray-oringway\n",
            "Epoch:  30 | Train loss: 1.039 | Val loss: 1.462 | Gen: ethay aysay-ay-ay-ay-away- oomsinglay-outionsda issway ounglyfay-inway-omat\n",
            "Epoch:  31 | Train loss: 1.031 | Val loss: 1.376 | Gen: ethay astay-inway-awlay ontonderssay-inway-a issway onglray-inway-anway\n",
            "Epoch:  32 | Train loss: 1.000 | Val loss: 1.314 | Gen: ethay aysay-ableway ontondsay-insay-omma issway onglray-oringway\n",
            "Epoch:  33 | Train loss: 0.986 | Val loss: 1.333 | Gen: ethay away-inway-anway ontonsray-insmay-awa issay onglray-inway-anway\n",
            "Epoch:  34 | Train loss: 0.983 | Val loss: 1.296 | Gen: ethay aysay-ableway onshingorishilay-and issway ounglyday-ommationda\n",
            "Epoch:  35 | Train loss: 0.963 | Val loss: 1.280 | Gen: ethay aysay-ay-ayway ontorishingsray issway onglray-inway-awlay\n",
            "Epoch:  36 | Train loss: 0.957 | Val loss: 1.270 | Gen: ethay aysay-ay-ayway onsingorshionshay issway onglray-inway-omaten\n",
            "Epoch:  37 | Train loss: 0.967 | Val loss: 1.314 | Gen: ethay aysay-andedway oomistingday-omation issay oouckay\n",
            "Epoch:  38 | Train loss: 0.957 | Val loss: 1.530 | Gen: ethay aishedway ontorisionsdshiongsa issway ongrorfay-ineway\n",
            "Epoch:  39 | Train loss: 0.982 | Val loss: 1.246 | Gen: ethay aray-away-away-away onshingrationgsay issway ounglyday\n",
            "Epoch:  40 | Train loss: 0.929 | Val loss: 1.265 | Gen: ethay away-inway-awlay onsidcistay-omationw issway ounglyfay\n",
            "Epoch:  41 | Train loss: 0.915 | Val loss: 1.280 | Gen: ethay ayslay ontorisshingsday issway ounglyfay\n",
            "Epoch:  42 | Train loss: 0.921 | Val loss: 1.281 | Gen: ethay away-inway-awlay ontrinsiondssday issway ongrlysay-andway\n",
            "Epoch:  43 | Train loss: 0.910 | Val loss: 1.280 | Gen: ethay ayshay ontorisshingsday issway ornglingway\n",
            "Epoch:  44 | Train loss: 0.894 | Val loss: 1.246 | Gen: ethay ayslay ontondsiongsday issway ounglyfay\n",
            "Epoch:  45 | Train loss: 0.895 | Val loss: 1.281 | Gen: ethay ayslay onsingormationsday issway onglousfay\n",
            "Epoch:  46 | Train loss: 0.932 | Val loss: 1.313 | Gen: ethay aysay onshictidcay-ondengw issway ounglyfay\n",
            "Epoch:  47 | Train loss: 0.916 | Val loss: 1.253 | Gen: ethay aingray onshiouringlay issway onglyday\n",
            "Epoch:  48 | Train loss: 0.877 | Val loss: 1.207 | Gen: ethay aysray onsidtingshiongay issway ounglyfay\n",
            "Epoch:  49 | Train loss: 0.855 | Val loss: 1.186 | Gen: ethay ayshay onsidtingray issway ornginglay\n",
            "Epoch:  50 | Train loss: 0.861 | Val loss: 1.213 | Gen: ethay aysray ontingshingshingshay issway ounglyfay\n",
            "Epoch:  51 | Train loss: 0.854 | Val loss: 1.229 | Gen: ethay ayslay onghintosshionshay issway ongrlionway\n",
            "Epoch:  52 | Train loss: 0.846 | Val loss: 1.230 | Gen: ethay aysray onshifitondatingway isway ounglyfay\n",
            "Epoch:  53 | Train loss: 0.836 | Val loss: 1.177 | Gen: ethay aysray ondintingshingway issway ornginglay\n",
            "Epoch:  54 | Train loss: 0.823 | Val loss: 1.169 | Gen: ethay aysay onsingorminghay isway ounglysay\n",
            "Epoch:  55 | Train loss: 0.812 | Val loss: 1.213 | Gen: etay ayslay onsidhingray-inway-a isway origlysay\n",
            "Epoch:  56 | Train loss: 0.810 | Val loss: 1.186 | Gen: ethay aysray onsinghidhingstray isway oungfray\n",
            "Epoch:  57 | Train loss: 0.810 | Val loss: 1.261 | Gen: ethay aislyway onsidcoushiongray isway ounflysay\n",
            "Epoch:  58 | Train loss: 0.844 | Val loss: 1.230 | Gen: etay ayslay onsidminglyshinglay isway origlysay\n",
            "Epoch:  59 | Train loss: 0.844 | Val loss: 1.192 | Gen: ethay ayslay onsidhingshay issway origray\n",
            "Epoch:  60 | Train loss: 0.793 | Val loss: 1.135 | Gen: ethay ayslay onsiddersshionsway isway ounglyfay\n",
            "Epoch:  61 | Train loss: 0.776 | Val loss: 1.171 | Gen: ethay ayslay onsidiourmay isway ounflyway\n",
            "Epoch:  62 | Train loss: 0.780 | Val loss: 1.194 | Gen: ethay ay-estay ontingdray-insmay-aw isway ounflypay\n",
            "Epoch:  63 | Train loss: 0.782 | Val loss: 1.286 | Gen: ethay ayslay onindishilyway isway ourginglay\n",
            "Epoch:  64 | Train loss: 0.788 | Val loss: 1.212 | Gen: ethay ayslay ontingdray-insay-ayd isway orkgingway\n",
            "Epoch:  65 | Train loss: 0.767 | Val loss: 1.161 | Gen: ethay ayslay onsingorminghay isway ounglyfay\n",
            "Epoch:  66 | Train loss: 0.766 | Val loss: 1.244 | Gen: ethay aysway onsinghtiongray-onwa isway oungfray\n",
            "Epoch:  67 | Train loss: 0.780 | Val loss: 1.219 | Gen: etay ayslay onsinghtiondsday isway orkingway\n",
            "Epoch:  68 | Train loss: 0.761 | Val loss: 1.203 | Gen: ethay ariway onsidhingray isway origlysay\n",
            "Epoch:  69 | Train loss: 0.756 | Val loss: 1.149 | Gen: ethay ayslay onsiditionday isway ounflyway\n",
            "Epoch:  70 | Train loss: 0.742 | Val loss: 1.166 | Gen: ethay arsway ondintingmay-answay isway oulfray\n",
            "Epoch:  71 | Train loss: 0.752 | Val loss: 1.235 | Gen: ethay ayslay onsingorminghay isway oulfingray\n",
            "Epoch:  72 | Train loss: 0.774 | Val loss: 1.283 | Gen: etway aysway onsingrorishiontway isway orkingsglay\n",
            "Epoch:  73 | Train loss: 0.769 | Val loss: 1.132 | Gen: ethay arsway onsiditybay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.728 | Val loss: 1.105 | Gen: ethay airway onsidtinghay-onsay-a isway orkingway\n",
            "Epoch:  75 | Train loss: 0.713 | Val loss: 1.100 | Gen: ethay airway onsiditionday isway orkingway\n",
            "Epoch:  76 | Train loss: 0.705 | Val loss: 1.072 | Gen: ethay airway ondintingshingway isway ouglycay\n",
            "Epoch:  77 | Train loss: 0.698 | Val loss: 1.100 | Gen: ethay aigay onsidhersshay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.699 | Val loss: 1.112 | Gen: ethay arsway ondintingshingbay isway oulgingray\n",
            "Epoch:  79 | Train loss: 0.713 | Val loss: 1.219 | Gen: ethay ayslay onsidtingday isway orkingway\n",
            "Epoch:  80 | Train loss: 0.752 | Val loss: 1.095 | Gen: ethay aigray onsinghtiondway isway oulgingday\n",
            "Epoch:  81 | Train loss: 0.706 | Val loss: 1.108 | Gen: ethay arsway onshidershay isway orkgway\n",
            "Epoch:  82 | Train loss: 0.698 | Val loss: 1.105 | Gen: ethay arsway onsidmersbay isway oulgingray\n",
            "Epoch:  83 | Train loss: 0.691 | Val loss: 1.167 | Gen: ethay arsway onhindersbay isway orkgingway\n",
            "Epoch:  84 | Train loss: 0.695 | Val loss: 1.096 | Gen: ethay aringpay ondintingmingshay isway oulfingray\n",
            "Epoch:  85 | Train loss: 0.682 | Val loss: 1.103 | Gen: ethay arsway onsidtondshay isway orkgingway\n",
            "Epoch:  86 | Train loss: 0.684 | Val loss: 1.067 | Gen: ethay ariway onsidhingray isway orkingway\n",
            "Epoch:  87 | Train loss: 0.673 | Val loss: 1.095 | Gen: ethay ariway ondintingshmay isway oulgingrray\n",
            "Epoch:  88 | Train loss: 0.676 | Val loss: 1.088 | Gen: ethay aintray onsorishingday isway orkinggay\n",
            "Epoch:  89 | Train loss: 0.676 | Val loss: 1.108 | Gen: ethay aringway ondintingrmay isway orkgingway\n",
            "Epoch:  90 | Train loss: 0.694 | Val loss: 1.076 | Gen: ethay airway onsindedtingshay isway ouglycybay\n",
            "Epoch:  91 | Train loss: 0.671 | Val loss: 1.101 | Gen: ethay ariway onsidtingbray isway ourkway\n",
            "Epoch:  92 | Train loss: 0.668 | Val loss: 1.119 | Gen: ethay ariway onhiningormitybay isway oulfingray\n",
            "Epoch:  93 | Train loss: 0.675 | Val loss: 1.104 | Gen: ethay ariway onsidedshay isway ouglivay\n",
            "Epoch:  94 | Train loss: 0.659 | Val loss: 1.079 | Gen: ethay ariway onsidtingbay isway oulfrway\n",
            "Epoch:  95 | Train loss: 0.650 | Val loss: 1.069 | Gen: ethay airway onintoidingsday isway oulgefray\n",
            "Epoch:  96 | Train loss: 0.647 | Val loss: 1.048 | Gen: ethay ariway onsidtoingray isway oulfrway\n",
            "Epoch:  97 | Train loss: 0.647 | Val loss: 1.120 | Gen: ethay aringpay ondintingshingbay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.670 | Val loss: 1.123 | Gen: ethay aringpay onsidtenfay-onsay isway orkgingway\n",
            "Epoch:  99 | Train loss: 0.673 | Val loss: 1.053 | Gen: ethay aisray onsiditiongray issway oulfreday\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay aisray onsiditiongray issway oulfreday\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE4ijaCzneAt",
        "colab_type": "text"
      },
      "source": [
        "Try translating different sentences by changing the variable TEST_SENTENCE. Identify two distinct failure modes and briefly describe them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrNnz8W1nULf",
        "colab_type": "code",
        "outputId": "a1a72550-65d7-4ea1-e99a-490ea0120857",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay aisray onsiditiongray issway oulfreday\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq67KiSjwaVu",
        "colab_type": "code",
        "outputId": "e9e5dfce-d828-463b-8759-b266ee6f574e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "TEST_SENTENCE = 'computer science'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tcomputer science \n",
            "translated:\topproustbay isesedsway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoGoLN9MxBfv",
        "colab_type": "code",
        "outputId": "def6cb35-731b-4668-a1f3-b56e50ee5aef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "TEST_SENTENCE = 'electric-powered'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\telectric-powered \n",
            "translated:\telacteray-ingecedway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLuHqW6oyeHG",
        "colab_type": "code",
        "outputId": "159e880f-fba8-40d8-c2d9-7210e59241d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "TEST_SENTENCE = 'to-buy'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tto-buy \n",
            "translated:\tolway-anway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWwA6OGqlaTq",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Additive Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJSafHSAmu_w",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Additive Attention\n",
        "Already implemented the additive attention mechanism. Write down the mathematical expression for $\\tilde{\\alpha}_i^{(t)}, \\alpha_i^{(t)}, c_t$ as a function of $W_1, W_2, b_1, b_2, Q_t, K_i$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdewEVSMo5jJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "                                    nn.Linear(hidden_size*2, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(hidden_size, 1)\n",
        "                                 )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "        batch_size = keys.size(0)\n",
        "        expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(keys)\n",
        "        concat_inputs = torch.cat([expanded_queries, keys], dim=2)\n",
        "        unnormalized_attention = self.attention_network(concat_inputs)\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights.transpose(2,1), values)\n",
        "        return context, attention_weights\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73_p8d5EmvOJ",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: RNN Additive Attention Decoder\n",
        "We will now implement a recurrent decoder that makes use of the additive attention mechanism. Read the description in the assignment worksheet and complete the following implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJaABkXrpJSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
        "        if attention_type == 'additive':\n",
        "          self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == 'scaled_dot':\n",
        "          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            # ------------\n",
        "            # FILL THIS IN - START\n",
        "            # ------------\n",
        "            embed_current = embed[:,i,:]  # Get the current time step, across the whole batch\n",
        "            context, attention_weights = self.attention(\n",
        "                h_prev, # queries @ (bs, hidden_size)\n",
        "                annotations, # keys @ (bs, sl, hs)\n",
        "                annotations # values @ (bs, sl, hs)\n",
        "            )  # @ (batch_size, 1,  hidden_size) and (batch_size, seq_len, 1)\n",
        "            embed_and_context = torch.cat((\n",
        "                embed_current.view(batch_size, -1),\n",
        "                context.view(batch_size, -1)),\n",
        "                dim=1\n",
        "            )  # batch_size x (2*hidden_size) \n",
        "            h_prev = self.rnn(embed_and_context, h_prev)  # batch_size x hidden_size \n",
        "            # ------------\n",
        "            # FILL THIS IN - END\n",
        "            # ------------    \n",
        "            \n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYPae08Io1Fi",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "Train the following language model that uses a recurrent encoder, and a recurrent decoder that has an additive attention component. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3-FuzY1pepu",
        "colab_type": "code",
        "outputId": "77dda4b4-e556-4978-9c20-a50ef68776bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'additive',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_attn_encoder, rnn_attn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 20                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn_attention                          \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('heirs', 'eirshay')\n",
            "('acquaintances', 'acquaintancesway')\n",
            "('uproar', 'uproarway')\n",
            "('mercy', 'ercymay')\n",
            "('indulgent', 'indulgentway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.286 | Val loss: 1.992 | Gen: elay-ay-ay-ay-ay-ay- ay-ay-ay-ay-ay-ay-ay ongay-ongay-ay-ay-ay onssssssssssssssssss ongay-ongay-ay-ay-ay\n",
            "Epoch:   1 | Train loss: 1.791 | Val loss: 1.736 | Gen: eday away ongay-ongay-ay-ay-ay issway ongay-ingay-ingay\n",
            "Epoch:   2 | Train loss: 1.543 | Val loss: 1.599 | Gen: eray atay-ay-ay-ay-ay-ay onsay-insay-ingay issway ongay-inday\n",
            "Epoch:   3 | Train loss: 1.366 | Val loss: 1.485 | Gen: eday atay-ay ongay-onway issway ongay-inway\n",
            "Epoch:   4 | Train loss: 1.201 | Val loss: 1.367 | Gen: eay-ay-ay-ay-ay-ay-a aray-ay-ay ongway-antinway isway ottingway\n",
            "Epoch:   5 | Train loss: 1.078 | Val loss: 1.298 | Gen: eateay aray-ay-ay onginway-inay-ingay- isway otringway-igray\n",
            "Epoch:   6 | Train loss: 0.955 | Val loss: 1.186 | Gen: eacay arway ondininningnway isway oorgray-ngray\n",
            "Epoch:   7 | Train loss: 0.825 | Val loss: 1.096 | Gen: epay arway ondininingnay-ingway isway orway-ingway\n",
            "Epoch:   8 | Train loss: 0.776 | Val loss: 1.296 | Gen: ecay arway onditiinanionininini isway orkinnatingway\n",
            "Epoch:   9 | Train loss: 0.769 | Val loss: 1.059 | Gen: ecay atay-ay onditiinininginningn isway othay-ingway\n",
            "Epoch:  10 | Train loss: 0.660 | Val loss: 0.861 | Gen: ecay airway onditiningway isway orkingway\n",
            "Epoch:  11 | Train loss: 0.582 | Val loss: 0.944 | Gen: ecay atay-ay onditindingway isway orkingway\n",
            "Epoch:  12 | Train loss: 0.544 | Val loss: 0.878 | Gen: ecay airway onditininingnway isway orkngnay\n",
            "Epoch:  13 | Train loss: 0.491 | Val loss: 0.742 | Gen: eway airway onditiininingway issway orkingway\n",
            "Epoch:  14 | Train loss: 0.459 | Val loss: 0.771 | Gen: etay airway onditindnablngway isway orkingway\n",
            "Epoch:  15 | Train loss: 0.453 | Val loss: 0.967 | Gen: eway atay onditionignway isway orignway\n",
            "Epoch:  16 | Train loss: 0.505 | Val loss: 0.882 | Gen: eway ailay onditiongnaray isway orkingway\n",
            "Epoch:  17 | Train loss: 0.447 | Val loss: 0.669 | Gen: eway airway onditionigngway isway orkngway\n",
            "Epoch:  18 | Train loss: 0.384 | Val loss: 1.143 | Gen: eway airway onditiningsray isway oringway\n",
            "Epoch:  19 | Train loss: 0.390 | Val loss: 0.516 | Gen: eway airway onditionignway isway orkngway\n",
            "Epoch:  20 | Train loss: 0.314 | Val loss: 0.600 | Gen: ehtay airway onditioningway isway orkngay\n",
            "Epoch:  21 | Train loss: 0.306 | Val loss: 0.441 | Gen: eway-etay airway onditioinignway isway orkingway\n",
            "Epoch:  22 | Train loss: 0.280 | Val loss: 0.429 | Gen: efhtay airway onditioningway isway orkingray\n",
            "Epoch:  23 | Train loss: 0.343 | Val loss: 0.687 | Gen: eway-etay airway onditiongway isway orkingway\n",
            "Epoch:  24 | Train loss: 0.314 | Val loss: 0.595 | Gen: eway-ettay airway onditioningway isway orkngnay\n",
            "Epoch:  25 | Train loss: 0.326 | Val loss: 0.598 | Gen: ethtay airway onditioningnay issay orkingway\n",
            "Epoch:  26 | Train loss: 0.247 | Val loss: 0.351 | Gen: eway-ettay airway onditionigfray isway orkingray\n",
            "Epoch:  27 | Train loss: 0.216 | Val loss: 0.350 | Gen: ethtay airway onditionignray isway orkingray\n",
            "Epoch:  28 | Train loss: 0.205 | Val loss: 0.294 | Gen: ethay-etay airway onditioningway isway orkingway\n",
            "Epoch:  29 | Train loss: 0.201 | Val loss: 0.457 | Gen: etay-aytay airway onditioningway isway orkingway\n",
            "Epoch:  30 | Train loss: 0.238 | Val loss: 0.411 | Gen: eway-ttay-tay-tay-ta airway onditioningway isway orkingway\n",
            "Epoch:  31 | Train loss: 0.208 | Val loss: 0.760 | Gen: eway-ttthay airway onditiongway isway orkingray\n",
            "Epoch:  32 | Train loss: 0.188 | Val loss: 0.370 | Gen: ewthay airway onditioningway isway orkingway\n",
            "Epoch:  33 | Train loss: 0.170 | Val loss: 0.342 | Gen: ewhay airway onditioningway isway orkingway\n",
            "Epoch:  34 | Train loss: 0.136 | Val loss: 0.241 | Gen: egay airway onditioningnay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.121 | Val loss: 0.497 | Gen: egay airway onditioningway isway orkingway\n",
            "Epoch:  36 | Train loss: 0.218 | Val loss: 1.086 | Gen: eway-ththalthyhrhay airway ondititioningsay isway orkikingway\n",
            "Epoch:  37 | Train loss: 0.263 | Val loss: 0.533 | Gen: egay airway onditiondway isway orkingway\n",
            "Epoch:  38 | Train loss: 0.148 | Val loss: 0.322 | Gen: eway-ththyhay airway onditioningway isway orkingway\n",
            "Epoch:  39 | Train loss: 0.109 | Val loss: 0.266 | Gen: egtay airway onditioningway isway orkingway\n",
            "Epoch:  40 | Train loss: 0.090 | Val loss: 0.196 | Gen: etay-ththay airway onditioningway isway orkingway\n",
            "Epoch:  41 | Train loss: 0.072 | Val loss: 0.204 | Gen: etay-ththay airway onditioningway isway orkingway\n",
            "Epoch:  42 | Train loss: 0.072 | Val loss: 0.219 | Gen: etay-thal airway onditioningway isway orkingway\n",
            "Epoch:  43 | Train loss: 0.068 | Val loss: 0.348 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  44 | Train loss: 0.089 | Val loss: 0.202 | Gen: ehtay airway onditioningway isway orkingway\n",
            "Epoch:  45 | Train loss: 0.191 | Val loss: 1.215 | Gen: ewthay airway ondfyonwablinay isway orkinway\n",
            "Epoch:  46 | Train loss: 0.220 | Val loss: 0.296 | Gen: etay airway onditionsingway isway orkingway\n",
            "Epoch:  47 | Train loss: 0.090 | Val loss: 0.226 | Gen: etay airway onditioningway isway orkingway\n",
            "Epoch:  48 | Train loss: 0.066 | Val loss: 0.214 | Gen: etay airway onditioningway isway orkingway\n",
            "Epoch:  49 | Train loss: 0.072 | Val loss: 0.400 | Gen: etay-ththay airway onditioningway isway orkingway\n",
            "Epoch:  50 | Train loss: 0.101 | Val loss: 0.662 | Gen: etay-ththyhay airway onditiongway isway orkingway\n",
            "Epoch:  51 | Train loss: 0.083 | Val loss: 0.203 | Gen: etay-tthay airway onditioningway isway orkingway\n",
            "Epoch:  52 | Train loss: 0.070 | Val loss: 0.263 | Gen: etay-ththay airway onditioningway isway orkingway\n",
            "Epoch:  53 | Train loss: 0.055 | Val loss: 0.523 | Gen: etay-ththay airway onditioiningway isway orkingway\n",
            "Epoch:  54 | Train loss: 0.053 | Val loss: 0.172 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.034 | Val loss: 0.158 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  56 | Train loss: 0.028 | Val loss: 0.153 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.026 | Val loss: 0.152 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  58 | Train loss: 0.023 | Val loss: 0.148 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.021 | Val loss: 0.149 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  60 | Train loss: 0.019 | Val loss: 0.146 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.018 | Val loss: 0.151 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  62 | Train loss: 0.017 | Val loss: 0.147 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.016 | Val loss: 0.151 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.019 | Val loss: 0.155 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.020 | Val loss: 0.195 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.055 | Val loss: 0.508 | Gen: egthay airway onditioningway isway orkingway\n",
            "Epoch:  67 | Train loss: 0.144 | Val loss: 0.358 | Gen: etay airway onditioningday isway orkingway\n",
            "Epoch:  68 | Train loss: 0.100 | Val loss: 0.322 | Gen: etay airway onditioningway isway orkingway\n",
            "Epoch:  69 | Train loss: 0.064 | Val loss: 0.206 | Gen: etay airway onditioniningway isway orkingway\n",
            "Epoch:  70 | Train loss: 0.043 | Val loss: 0.157 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.025 | Val loss: 0.147 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.019 | Val loss: 0.141 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.015 | Val loss: 0.138 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.013 | Val loss: 0.134 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.012 | Val loss: 0.135 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.011 | Val loss: 0.133 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.010 | Val loss: 0.134 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.009 | Val loss: 0.132 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.009 | Val loss: 0.136 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.008 | Val loss: 0.128 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.008 | Val loss: 0.138 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.015 | Val loss: 1.393 | Gen: ethay airway ondieningcay isway orkiwfay\n",
            "Epoch:  83 | Train loss: 0.200 | Val loss: 0.279 | Gen: ettay airway onditioningway isway orkingway\n",
            "Epoch:  84 | Train loss: 0.049 | Val loss: 0.209 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.029 | Val loss: 0.166 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.017 | Val loss: 0.163 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.013 | Val loss: 0.157 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.010 | Val loss: 0.149 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.009 | Val loss: 0.143 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.008 | Val loss: 0.137 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.007 | Val loss: 0.137 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.007 | Val loss: 0.134 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.006 | Val loss: 0.135 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.006 | Val loss: 0.128 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.006 | Val loss: 0.138 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.006 | Val loss: 0.133 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.006 | Val loss: 0.137 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.056 | Val loss: 1.192 | Gen: ethay airway ondicecgcay isway orkiwway\n",
            "Epoch:  99 | Train loss: 0.123 | Val loss: 0.332 | Gen: ethay airway onditionwway isway orkingway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditionwway isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNVKbLc0ACj_",
        "colab_type": "code",
        "outputId": "2d5461dc-eb20-4a2f-a4f8-c0d9c4c508fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditionwway isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw_GOIvzo1ix",
        "colab_type": "text"
      },
      "source": [
        "# Part 3: Scaled Dot Product Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq7nhsEio1w-",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Implement Dot-Product Attention\n",
        "Implement the scaled dot product attention module described in the assignment worksheet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_j3oY3hqsJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size, seq_len, k) # changed\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        hidden_size = self.hidden_size\n",
        "        batch_size = queries.shape[0]\n",
        "        d = hidden_size\n",
        "        # Convert tensor to 3D.\n",
        "        # k is the number of queries.\n",
        "        queries = queries.view(batch_size, -1, hidden_size)\n",
        "        num_queries = queries.shape[1]\n",
        "        seq_len = keys.shape[1]\n",
        "        # Expand.\n",
        "        # keys = keys.expand(batch_size, seq_len, hidden_size)\n",
        "        # keys = torch.transpose(keys, dim0=0, dim1=1)\n",
        "\n",
        "        q = self.Q(queries) # @ (batch_size, k, hidden_size)\n",
        "        k = self.K(keys) # @ (batch_size, seq_len, hidden_size)\n",
        "        v = self.V(values) # @ (batch_size, seq_len, hidden_size)\n",
        "        q = torch.transpose(q, 1, 2) # @ (batch_size, hidden_size, k)\n",
        "        # print(\"q @\", q.shape)\n",
        "        # print(\"k @\", k.shape)\n",
        "        unnormalized_attention = torch.bmm(k, q) * self.scaling_factor\n",
        "        # unnormalized_attention @ (batch_size, seq_len, k)\n",
        "        # print(unnormalized_attention.shape)\n",
        "    \n",
        "        attention_weights = self.softmax(unnormalized_attention).transpose(1, 2) # @ (batch_size, k, seq_len)\n",
        "        context = torch.bmm(attention_weights, v) # @ (batch_size, k, hidden_size)\n",
        "        attention_weights = attention_weights.transpose(1, 2) # @ (batch_size, seq_len, k)\n",
        "        return context, attention_weights\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA8oTAPqyv8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test code.\n",
        "att = ScaledDotAttention(20)\n",
        "annot = torch.randn(64, 10, 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EcH2Luny8Zh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c, a = att(annot, annot, annot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unReAOrjo113",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Implement Causal Dot-Product Attention\n",
        "Now implement the scaled causal dot product described in the assignment worksheet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovigzQffrKqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        hidden_size = self.hidden_size\n",
        "        batch_size = queries.shape[0]\n",
        "        d = hidden_size\n",
        "        # Convert tensor to 3D.\n",
        "        # k is the number of queries.\n",
        "        queries = queries.view(batch_size, -1, hidden_size)\n",
        "        num_queries = queries.shape[1]\n",
        "        seq_len = keys.shape[1]\n",
        "        # keys = keys.expand(batch_size, seq_len, hidden_size)\n",
        "        # keys = torch.transpose(keys, dim0=0, dim1=1)\n",
        "\n",
        "        q = self.Q(queries) # @ (batch_size, k, hidden_size)\n",
        "        k = self.K(keys) # @ (batch_size, seq_len, hidden_size)\n",
        "        v = self.V(values) # @ (batch_size, seq_len, hidden_size)\n",
        "        q = torch.transpose(q, 2, 1) # @ (batch_size, hidden_size, k)\n",
        "        # print(\"q @\", q.shape)\n",
        "        # print(\"k @\", k.shape)\n",
        "        unnormalized_attention = torch.bmm(k, q) * self.scaling_factor\n",
        "        # unnormalized_attention @ (batch_size, seq_len, k)\n",
        "        # print(unnormalized_attention.shape)\n",
        "        # ==== Enforce Casual ====\n",
        "        mask = torch.tril(torch.ones_like(unnormalized_attention)) * self.neg_inf\n",
        "        unnormalized_attention += mask\n",
        "        # ==== End ====\n",
        "        attention_weights = self.softmax(unnormalized_attention).transpose(1, 2) # @ (batch_size, k, seq_len)\n",
        "        context = torch.bmm(attention_weights, v) # @ (batch_size, k, hidden_size)\n",
        "        attention_weights = attention_weights.transpose(1, 2) # @ (batch_size, seq_len, k)\n",
        "        return context, attention_weights\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tcpUFKqo2Oi",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Transformer Encoder\n",
        "Complete the following transformer encoder implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3B-fWsarlVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        \n",
        "        # IMPORTANT CORRECTION: NON-CAUSAL ATTENTION SHOULD HAVE BEEN\n",
        "        # USED IN THE TRANSFORMER ENCODER. \n",
        "        # NEW VERSION: \n",
        "        self.self_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        # PREVIONS VERSION: \n",
        "        # self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
        "        #                             hidden_size=hidden_size, \n",
        "        #                          ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        # ------------\n",
        "        # FILL THIS IN - START\n",
        "        # ------------\n",
        "        encoded = self.embedding(inputs)  # @ (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n",
        "        # self.positional_encodings @ (max_seq_len, hidden_size)\n",
        "        # Get the first few positional encodings.\n",
        "        # pos = self.positional_encodings[:seq_len, :] # @ (seq_len, hidden_size)\n",
        "        # Expand to batches.\n",
        "        # pos = pos.expand(batch_size, seq_len, self.hidden_size)\n",
        "        encoded = encoded + self.positional_encodings[:seq_len]\n",
        "\n",
        "        # Initiate hidden states, the first hidden state is the embedded input layer.\n",
        "        annotations = encoded # @ (batch_size, seq_len, hidden_size)\n",
        "        # ==== Shapes ====\n",
        "        # print(inputs.shape) # 64, 10\n",
        "        # print(self.embedding(inputs).shape) # 64, 10, 20\n",
        "        # print(annotations.shape) # 64, 10, 20\n",
        "        # ==== End ====\n",
        "        for i in range(self.num_layers):\n",
        "            new_annotations, self_attention_weights = self.self_attentions[i](\n",
        "                annotations, annotations, annotations\n",
        "            )  # batch_size x seq_len x hidden_size\n",
        "            # annotation with residual added.\n",
        "            residual_annotations = annotations + new_annotations\n",
        "            new_annotations = self.attention_mlps[i](residual_annotations)\n",
        "            # Update annotations, the output of this layer.\n",
        "            annotations = residual_annotations + new_annotations\n",
        "        # ------------\n",
        "        # FILL THIS IN - END\n",
        "        # ------------\n",
        "\n",
        "        # Transformer encoder does not have a last hidden layer. \n",
        "        return annotations, None  \n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "      \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "      Arguments:\n",
        "          max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "      Returns:\n",
        "          pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
        "      \"\"\"\n",
        "      pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "      dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
        "      exponents = (2*dim_indices).float()/(self.hidden_size)\n",
        "      trig_args = pos_indices / (10000**exponents)\n",
        "      sin_terms = torch.sin(trig_args)\n",
        "      cos_terms = torch.cos(trig_args)\n",
        "\n",
        "      pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "      pos_encodings[:, 0::2] = sin_terms\n",
        "      pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "      if self.opts.cuda:\n",
        "        pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "      return pos_encodings\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1hDi020rT36",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Transformer Decoder\n",
        "Complete the following transformer decoder implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyvTZFxtrvc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Correct setting.        \n",
        "        self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "\n",
        "        # Experimental changes\n",
        "        # self.self_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "        #                             hidden_size=hidden_size, \n",
        "        #                          ) for i in range(self.num_layers)])\n",
        "\n",
        "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size \n",
        "\n",
        "        # THIS LINE WAS ADDED AS A CORRECTION. \n",
        "        embed = embed + self.positional_encodings[:seq_len]       \n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "\n",
        "        # Decoder: the input fed to the first layer.\n",
        "        contexts = embed # batch_size x seq_len x hidden_size \n",
        "        for i in range(self.num_layers):\n",
        "            # ------------\n",
        "            # FILL THIS IN - START\n",
        "            # ------------\n",
        "            new_contexts, self_attention_weights = self.self_attentions[i](\n",
        "                contexts, contexts, contexts\n",
        "            ) # batch_size x seq_len x hidden_size\n",
        "            # print(annotations.shape)\n",
        "            # print(new_contexts.shape)\n",
        "            # print(contexts.shape)\n",
        "            residual_contexts = contexts + new_contexts\n",
        "            new_contexts, encoder_attention_weights = self.encoder_attentions[i](\n",
        "                residual_contexts, annotations, annotations\n",
        "            ) # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = residual_contexts + new_contexts\n",
        "            new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "            contexts = residual_contexts + new_contexts\n",
        "\n",
        "            # ------------\n",
        "            # FILL THIS IN - END\n",
        "            # ------------\n",
        "          \n",
        "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "            self_attention_weights_list.append(self_attention_weights)\n",
        "          \n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "        \n",
        "        return output, (encoder_attention_weights, self_attention_weights)\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "      \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "      Arguments:\n",
        "          max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "      Returns:\n",
        "          pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
        "      \"\"\"\n",
        "      pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "      dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
        "      exponents = (2*dim_indices).float()/(self.hidden_size)\n",
        "      trig_args = pos_indices / (10000**exponents)\n",
        "      sin_terms = torch.sin(trig_args)\n",
        "      cos_terms = torch.cos(trig_args)\n",
        "\n",
        "      pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "      pos_encodings[:, 0::2] = sin_terms\n",
        "      pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "      pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "      return pos_encodings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ZjkXTNrUKb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Step 5: Training and analysis\n",
        "Now, train the following language model that's comprised of a (simplified) transformer encoder and transformer decoder. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmoTgrDcr_dw",
        "colab_type": "code",
        "outputId": "95f46bdd-7bf6-4007-cc30-e45ede90e2ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005,  ## INCREASE BY AN ORDER OF MAGNITUDE\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "transformer_encoder, transformer_ecoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 20                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('heirs', 'eirshay')\n",
            "('acquaintances', 'acquaintancesway')\n",
            "('uproar', 'uproarway')\n",
            "('mercy', 'ercymay')\n",
            "('indulgent', 'indulgentway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.158 | Val loss: 1.996 | Gen: d away--y ongay-ny ydday yraEOS--yyy\n",
            "Epoch:   1 | Train loss: 1.650 | Val loss: 1.987 | Gen: away-iway ay-bay-ibay ongay-aiiwa-aiy isiswiiaiaagiyiiiiii ada-ay\n",
            "Epoch:   2 | Train loss: 1.473 | Val loss: 1.947 | Gen: -------------------- errayray-erarry ingngnaiiintnay esineweapa-aaaiay --------------------\n",
            "Epoch:   3 | Train loss: 1.326 | Val loss: 1.618 | Gen: awaay-eay aay onngngngngntiiongnnn isy oray-ingay\n",
            "Epoch:   4 | Train loss: 1.222 | Val loss: 1.681 | Gen: ywwwywway yay oncifiiiiiaanayy yay-wwayy ydaayngy\n",
            "Epoch:   5 | Train loss: 1.100 | Val loss: 1.556 | Gen: --  ondfingsayEOSntttEOSEOSaaa  ala-inglalay\n",
            "Epoch:   6 | Train loss: 0.972 | Val loss: 1.292 | Gen: etay iay idtit-iittaay iaayy anngagwawy\n",
            "Epoch:   7 | Train loss: 0.877 | Val loss: 1.404 | Gen: eway-wwwaayy awaawraay inday-inday wayEOSwawway orkin-ayyEOSiingy\n",
            "Epoch:   8 | Train loss: 0.859 | Val loss: 1.212 | Gen: -------------------- EOSr ondutingogaay -------------------- --------------------\n",
            "Epoch:   9 | Train loss: 0.717 | Val loss: 1.150 | Gen: ewwwaay awwwwawawawwwawawwwa onditingoy-ingoy-ind awwwaayyEOSaay owwwayinwyy\n",
            "Epoch:  10 | Train loss: 0.655 | Val loss: 1.109 | Gen: ehayEOSethhay iarr-iiaaay ondcay-ingngayy isayEOSiissayayyyyEOSyay orkaanny\n",
            "Epoch:  11 | Train loss: 0.655 | Val loss: 1.558 | Gen: ewwawaay iaay inaliiynnnynnnnnnddd iay orkinnnay\n",
            "Epoch:  12 | Train loss: 0.717 | Val loss: 0.989 | Gen: --  onddiinginnnaoyy wwayEOSy --\n",
            "Epoch:  13 | Train loss: 0.510 | Val loss: 0.973 | Gen: ehaay aiai-EOSaaawy onditintiontiondwina awiiiiaayyyEOSaaaaaayy orkinggayy\n",
            "Epoch:  14 | Train loss: 0.510 | Val loss: 1.024 | Gen: -- arwwaaay ondadiaaaaoooooyyy iwwwaay orkinggay\n",
            "Epoch:  15 | Train loss: 0.465 | Val loss: 0.786 | Gen: ewwwaay irayy onddwtwyaaayyywEOSEOSwwa awwwaaaay orkingway\n",
            "Epoch:  16 | Train loss: 0.367 | Val loss: 0.716 | Gen: ehhay irray onditiningngngay isswswwwwwaay ariningngrray\n",
            "Epoch:  17 | Train loss: 0.416 | Val loss: 1.137 | Gen: ehthha-ay-wrEOSy -- wndddioinnggnwwwaawa -- --\n",
            "Epoch:  18 | Train loss: 0.462 | Val loss: 0.926 | Gen: eaayy irrwway onditiontinniany iwswaayywyyEOSyEOSaEOSayyy arinigbay\n",
            "Epoch:  19 | Train loss: 0.387 | Val loss: 0.742 | Gen: ewway irwaay ooniinnnntngioy issway orkiagyaEOSay\n",
            "Epoch:  20 | Train loss: 0.324 | Val loss: 0.746 | Gen: ehhay iiritaiaay ondidtiioinngny iswwiiiwwayy orbingbay\n",
            "Epoch:  21 | Train loss: 0.256 | Val loss: 0.649 | Gen: ehhaay iiriaay onditiniiiinnnaay ayEOSwiiaayyEOSEOSaaay arkinnwwwaayy\n",
            "Epoch:  22 | Train loss: 0.197 | Val loss: 0.534 | Gen: ehtay irrway onditiongcaayyyyEOSaaa isasay orkkiggaay\n",
            "Epoch:  23 | Train loss: 0.222 | Val loss: 0.900 | Gen: e-ay iwway-EOSaway ondidionontnggy issay arfinggnwnwaway\n",
            "Epoch:  24 | Train loss: 0.268 | Val loss: 0.775 | Gen: ewtaay irraaay onditiiiinnnniicyEOSEOSo isisisisisisisway orkingngowwway\n",
            "Epoch:  25 | Train loss: 0.232 | Val loss: 0.709 | Gen: qhay iielay ondittonniiiiiaay isaayy ykninnggayy\n",
            "Epoch:  26 | Train loss: 0.272 | Val loss: 0.493 | Gen: ehtay iirray onditinongnggcyy isay okkk-kway\n",
            "Epoch:  27 | Train loss: 0.182 | Val loss: 0.490 | Gen: ettway iirwwaay onditinacacayyEOSiiina isiiiifiiiaiy orkingnay\n",
            "Epoch:  28 | Train loss: 0.223 | Val loss: 0.836 | Gen: ehhay airrway onndisioniinnayy issay oawliggway\n",
            "Epoch:  29 | Train loss: 0.464 | Val loss: 0.935 | Gen: ehtay aiwway onditioingitiiggciEOSo -- orkinggbay\n",
            "Epoch:  30 | Train loss: 0.319 | Val loss: 0.434 | Gen: ethaay iray onditioaoinnnnyy isway orkingbay\n",
            "Epoch:  31 | Train loss: 0.160 | Val loss: 0.522 | Gen: eaaaaayy iiaraay ondiotnnnggiicay isaay oorknngay\n",
            "Epoch:  32 | Train loss: 0.217 | Val loss: 0.478 | Gen: eaay-atthhay iarrway onditionongcanay iswaaiyay ikeinnbay\n",
            "Epoch:  33 | Train loss: 0.161 | Val loss: 0.407 | Gen: etnay awraywyraaaaaaaEOSEOSEOSaa onditionongncayyy isaay orkiigway\n",
            "Epoch:  34 | Train loss: 0.162 | Val loss: 0.614 | Gen: ehaay irwaay ondioioonnnggway iiay orringbay\n",
            "Epoch:  35 | Train loss: 0.166 | Val loss: 0.482 | Gen: ewaay iarway onditionongcayyy isway owwibgwaay\n",
            "Epoch:  36 | Train loss: 0.143 | Val loss: 0.439 | Gen: etthay irrway onditioniniiaacay isway orkingray\n",
            "Epoch:  37 | Train loss: 0.107 | Val loss: 0.340 | Gen: etayEOSEOSyyay iirway onditionnggcayEOSEOSay isaay orkingway\n",
            "Epoch:  38 | Train loss: 0.075 | Val loss: 0.420 | Gen: ehay iirway onditioniionnnay isiay orkinggway\n",
            "Epoch:  39 | Train loss: 0.080 | Val loss: 0.334 | Gen: etthay iirway onditioningcaay isway okrinnway\n",
            "Epoch:  40 | Train loss: 0.171 | Val loss: 0.730 | Gen: etayhaay iirwwaywaaway ondioioonnnnngy iissaayy ourkingway\n",
            "Epoch:  41 | Train loss: 0.138 | Val loss: 0.592 | Gen: eetay iirlay onditiiniiinnncay isaayay orkkingway\n",
            "Epoch:  42 | Train loss: 0.187 | Val loss: 0.491 | Gen: ehtay awaay ondittiionngcay isway orray\n",
            "Epoch:  43 | Train loss: 0.137 | Val loss: 0.310 | Gen: ethay iarway onditionngccay isway owringbay\n",
            "Epoch:  44 | Train loss: 0.075 | Val loss: 0.336 | Gen: ettaay iarayy onditionongnayy isaay orkikgwaay\n",
            "Epoch:  45 | Train loss: 0.059 | Val loss: 0.261 | Gen: ettay irrway onditionoigcay isaay orkingwway\n",
            "Epoch:  46 | Train loss: 0.044 | Val loss: 0.297 | Gen: eethay iurway onditionngccay isaaaysaay orkingwway\n",
            "Epoch:  47 | Train loss: 0.042 | Val loss: 0.303 | Gen: ehthy iirway onditiononnntay iswyyyEOSiaaaaay orkingwaay\n",
            "Epoch:  48 | Train loss: 0.096 | Val loss: 0.492 | Gen: ethhay irrbay onditionggiinaayy isiaiasisaay orkkngnay\n",
            "Epoch:  49 | Train loss: 0.207 | Val loss: 1.175 | Gen: -- ayi-ydyby ondtinnngggeeery -- --\n",
            "Epoch:  50 | Train loss: 0.276 | Val loss: 0.838 | Gen: ehhay airwaay onditionondcay awway orkingway\n",
            "Epoch:  51 | Train loss: 0.178 | Val loss: 0.405 | Gen: ettay airwway onditioningincy isswaay orkikggay\n",
            "Epoch:  52 | Train loss: 0.096 | Val loss: 0.274 | Gen: etthay iirwaay onditionongcaay isisisissaay orkingwway\n",
            "Epoch:  53 | Train loss: 0.056 | Val loss: 0.301 | Gen: etthay aierway onditioningggy isisisissaay orkingwway\n",
            "Epoch:  54 | Train loss: 0.081 | Val loss: 0.276 | Gen: ethhay iirway onditioningcayy isaaay orkingway\n",
            "Epoch:  55 | Train loss: 0.047 | Val loss: 0.268 | Gen: ethaay iieeway onditioningcay ibsaay orkinggay\n",
            "Epoch:  56 | Train loss: 0.041 | Val loss: 0.269 | Gen: ethhay iirway onditioningcay issswwwwwwaay orkingway\n",
            "Epoch:  57 | Train loss: 0.033 | Val loss: 0.326 | Gen: etthay iirwaay onditioningcay isaaay orkingway\n",
            "Epoch:  58 | Train loss: 0.065 | Val loss: 0.455 | Gen: ehthay ibrway onditinoingcay iswwway orkingwway\n",
            "Epoch:  59 | Train loss: 0.087 | Val loss: 0.356 | Gen: etwaay iirbway onditionongcayy issisisissway orkingway\n",
            "Epoch:  60 | Train loss: 0.073 | Val loss: 0.355 | Gen: etaay iarway onditionengway isisiaiasy orkingwway\n",
            "Epoch:  61 | Train loss: 0.057 | Val loss: 0.289 | Gen: ethay iirbway onditioningeway iswayyiswyyyyywwwaaa orkingbaay\n",
            "Epoch:  62 | Train loss: 0.036 | Val loss: 0.241 | Gen: ethhay iirway onditioningcay isisisiisiway orkingwaay\n",
            "Epoch:  63 | Train loss: 0.018 | Val loss: 0.262 | Gen: etthay iirway onditioningcayy isisssisiiy orkingwaay\n",
            "Epoch:  64 | Train loss: 0.012 | Val loss: 0.215 | Gen: etthay ibrwway onditioningcaay issswswswwaay orkingwaay\n",
            "Epoch:  65 | Train loss: 0.021 | Val loss: 0.334 | Gen: ettay iriwaay onditioniincay iswsswwwwayyywaaaayy orkingwaay\n",
            "Epoch:  66 | Train loss: 0.032 | Val loss: 0.491 | Gen: eetaaay iiriepEOSiay onditioningcayy issswwswwwaay orkingwaay\n",
            "Epoch:  67 | Train loss: 0.046 | Val loss: 0.403 | Gen: eethay iirraay onditioninngray isississiiiiiiiwy orkingbway\n",
            "Epoch:  68 | Train loss: 0.116 | Val loss: 0.806 | Gen: etaday iirrwway inay iii- orwinnrway\n",
            "Epoch:  69 | Train loss: 0.193 | Val loss: 0.872 | Gen: ettaa-ahhhyy awwwway ondnitttinngggcy awwwwwwwwway orkkiigway\n",
            "Epoch:  70 | Train loss: 0.135 | Val loss: 0.293 | Gen: etwaay iirway onditioningcay iswwaay orkinggway\n",
            "Epoch:  71 | Train loss: 0.041 | Val loss: 0.223 | Gen: etwaay iirway onditioningcay iswaay orkingwaay\n",
            "Epoch:  72 | Train loss: 0.022 | Val loss: 0.256 | Gen: ettay iirway onditioningcay isiiaaiiaawy orkingwaay\n",
            "Epoch:  73 | Train loss: 0.020 | Val loss: 0.303 | Gen: ethhay iurwayy onditioningcay isiisiiiiwiyEOSEOSyy orkingwway\n",
            "Epoch:  74 | Train loss: 0.015 | Val loss: 0.228 | Gen: etwway ibrway onditioningcay iii- orkingway\n",
            "Epoch:  75 | Train loss: 0.009 | Val loss: 0.202 | Gen: ettway irrwaay onditioningcay iswwwwwwwwaay orkingwaay\n",
            "Epoch:  76 | Train loss: 0.007 | Val loss: 0.213 | Gen: etwway irrway onditioningcay iswsaiiiiiy orkingway\n",
            "Epoch:  77 | Train loss: 0.006 | Val loss: 0.200 | Gen: ettway irrway onditioningcay iswwwwiiaaay orkingway\n",
            "Epoch:  78 | Train loss: 0.004 | Val loss: 0.194 | Gen: ethhay irrway onditioningcay isissiiiiy orkingway\n",
            "Epoch:  79 | Train loss: 0.003 | Val loss: 0.202 | Gen: ettway irrway onditioningcay isissiiiiy orkingway\n",
            "Epoch:  80 | Train loss: 0.003 | Val loss: 0.238 | Gen: etthay irrway onditioningcay isswiiy orkingway\n",
            "Epoch:  81 | Train loss: 0.007 | Val loss: 0.190 | Gen: ethhay irrway onditioningcay iswsaisaiiay orkingway\n",
            "Epoch:  82 | Train loss: 0.004 | Val loss: 0.215 | Gen: etwway ibrway onditioningcay iswwwwwaaaay orkingway\n",
            "Epoch:  83 | Train loss: 0.022 | Val loss: 0.392 | Gen: ethhay iariayy onditiionnnnnly iii- owray\n",
            "Epoch:  84 | Train loss: 0.087 | Val loss: 0.433 | Gen: ettay irwwway onditioniiggry iiiiaay orkinngway\n",
            "Epoch:  85 | Train loss: 0.135 | Val loss: 0.677 | Gen: -ataayyy irraay onditioningggwlaEOSy iisiisiiy orkingwaay\n",
            "Epoch:  86 | Train loss: 0.171 | Val loss: 0.382 | Gen: ehayay irwaay onditinoningcy isway orkingway\n",
            "Epoch:  87 | Train loss: 0.062 | Val loss: 0.261 | Gen: etwhay awray onditioningcay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.035 | Val loss: 0.250 | Gen: eetay iarway onditioningcayy iay orkingwayy\n",
            "Epoch:  89 | Train loss: 0.031 | Val loss: 0.236 | Gen: ethaay iirway onditioningcayy iswway orkingwaay\n",
            "Epoch:  90 | Train loss: 0.021 | Val loss: 0.184 | Gen: ethhay iirbay onditioningcay ii- orkingwway\n",
            "Epoch:  91 | Train loss: 0.010 | Val loss: 0.209 | Gen: etthay iirway onditioningcay ii- orkingwway\n",
            "Epoch:  92 | Train loss: 0.007 | Val loss: 0.159 | Gen: ethhay iirway onditioningcay issaaiasiiiy orkingwaay\n",
            "Epoch:  93 | Train loss: 0.004 | Val loss: 0.154 | Gen: ethhay iarway onditioningcay isayyiiissaaay orkingway\n",
            "Epoch:  94 | Train loss: 0.003 | Val loss: 0.169 | Gen: etthay iarway onditioningcay isiasiiay orkingway\n",
            "Epoch:  95 | Train loss: 0.002 | Val loss: 0.166 | Gen: ethhay iarway onditioningcay iseway orkingway\n",
            "Epoch:  96 | Train loss: 0.002 | Val loss: 0.180 | Gen: ethhay iirway onditioningcay isiiiiiiiiiissssacy orkingwaay\n",
            "Epoch:  97 | Train loss: 0.002 | Val loss: 0.175 | Gen: ethhay iirway onditioningcay iswway orkingway\n",
            "Epoch:  98 | Train loss: 0.002 | Val loss: 0.171 | Gen: ethhay iirway onditioningcay iswway orkingway\n",
            "Epoch:  99 | Train loss: 0.001 | Val loss: 0.182 | Gen: ethhay iirway onditioningcay iswway orkingway\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-aea35cc94602>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_ecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_SENTENCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"source:\\t\\t{} \\ntranslated:\\t{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_SENTENCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transformer_decoder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R18s80gzC6A8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBnBXRG8mvcn",
        "colab_type": "text"
      },
      "source": [
        "# Optional: Attention Visualizations\n",
        "\n",
        "One of the benefits of using attention is that it allows us to gain insight into the inner workings of the model.\n",
        "\n",
        "By visualizing the attention weights generated for the input tokens in each decoder step, we can see where the model focuses while producing each output token.\n",
        "\n",
        "The code in this section loads the model you trained from the previous section and uses it to translate a given set of words: it prints the translations and display heatmaps to show how attention is used at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqEC0vN9mvpV",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Visualize Attention Masks\n",
        "Play around with visualizing attention maps generated by the previous two models you've trained. Inspect visualizations in one success and one failure case for both models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkfz-u-MtudL",
        "colab_type": "code",
        "outputId": "ff9d6c63-9cfe-4c47-ae2f-dfdd0abc7d90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "TEST_WORD_ATTN = 'street'\n",
        "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-5075212dbc3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTEST_WORD_ATTN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'street'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvisualize_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_WORD_ATTN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_attn_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_attn_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'rnn_attn_encoder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ssa7g35zt2yj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST_WORD_ATTN = 'street'\n",
        "visualize_attention(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args, )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owstslMF-wdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}